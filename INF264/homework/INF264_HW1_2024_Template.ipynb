{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1 – $k$-NN Classifier\n",
    "\n",
    "Welcome to the first homework in INF264 where we will explore the $k$ nearest neighbors ($k$-NN) classifier.\n",
    "\n",
    "Since this is the first homework, we are going to be real nice and provide partial solutions. The parts where you need to do some work are indicated with a comment `#TODO!` in the code.\n",
    "\n",
    "#### Overview\n",
    "\n",
    "1. In the first part, we will use a implementation of a $k$-NN classifier from `sklearn`. We will use validation data to pick the optimal value for $k$ and in the end evaluate our final classifier on some unseen test data.\n",
    "\n",
    "2. In the second part, we will visualize how our classifier makes decisions. We will do this for different values of $k$ and try to understand how $k$ affects the decision making.\n",
    "\n",
    "3. That third and last part is a bonus exercise where you implement your own $k$-NN classifier from scratch.\n",
    "\n",
    "Let us start by importing NumPy and Matplotlib. We will need these for sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 – Training and evaluating a $k$-NN classifier using `sklearn`\n",
    "\n",
    "In this first part, we are going to use the `sklearn` implementation of a $k$-NN classifier to classify different species of the flower Iris based on two measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load the Iris dataset using `load_iris()` from `sklearn`. This returns a Python dictionary containing the dataset. The most important keys in this dictionary are:\n",
    "\n",
    "- `data` : A 2-dimensional floating point NumPy array of shape (150, 4) containing the features of individual Iris flowers (each row corresponds to a single flower and the different columns correspond to the four measurements).\n",
    "- `target` :  A 1-dimensional integer NumPy array of shape (150,) containing the labels (the type of Iris flower).\n",
    "- `target_names` : A list of strings of the class names `[\"setosa\" \"versicolor\" \"virginica\"]`.\n",
    "- `feature_names` : A list of strings of the feature names `[\"sepal length (cm)\", \"sepal width (cm)\", \"petal length (cm)\", \"petal width (cm)\"]`.\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Import the function `load_iris` from the module `sklearn.datasets`.\n",
    "2. Call `load_iris()` and store the dataset in a new variable called `dataset`.\n",
    "3. Store the two first columns of `dataset.data` in a variable `X` (our features).\n",
    "4. Store the labels `dataset.target` in a variable `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Iris dataset with features:\n",
      "\t['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "and targets:\n",
      "\t['setosa' 'versicolor' 'virginica']\n",
      "X has shape (150, 4), and y has shape (150,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris;\n",
    "\n",
    "dataset = load_iris() \n",
    "\n",
    "print(f\"Loaded Iris dataset with features:\\n\\t{dataset.feature_names}\\nand targets:\\n\\t{dataset.target_names}\")\n",
    "\n",
    "X = dataset.data #TODO! Extract the first two columns/features from the dataset\n",
    "y = dataset.target #TODO! Extract the target values from the dataset\n",
    "\n",
    "print(f\"X has shape {X.shape}, and y has shape {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to split the dataset into three smaller ones. Namely, training data, validation data and testing data. These serves the following purposes:\n",
    "\n",
    "- **Training data** is used to train our $k$NN classifier (learning).\n",
    "- **Validation data** is used to tune the classifier (finding the best value for $k$ in this case).\n",
    "- **Test data** is not used during the training and is used to evaluate the final classifier on unseen data (giving us an estimate of how well our classifier will perform on new data).\n",
    "\n",
    "**Your tasks:**\n",
    "Split the dataset into 70% training data, and 15% for both validation and test data. To achieve this, we will use `train_test_split()` ([docs](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)) from `sklearn` twice.\n",
    "\n",
    "Features for training, validation and test data should be stored in variables named `X_train`, `X_val` and `X_test`, respectively. The labels/targets should be stored in variables named `y_train`, `y_val` and `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 105 rows.\n",
      "Validation data: 23 rows.\n",
      "Testing (unseen) data: 22 rows.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(\n",
    "    X, y, test_size=0.3\n",
    ") #TODO! Split the data into training and testing+validation sets\n",
    "X_test, X_val, y_test, y_val = train_test_split(\n",
    "    X_val_test, y_val_test, test_size=0.5\n",
    ") #TODO! Split the testing+validation set into testing and validation sets\n",
    "\n",
    "print(f\"Training data: {len(X_train)} rows.\")\n",
    "print(f\"Validation data: {len(X_val)} rows.\")\n",
    "print(f\"Testing (unseen) data: {len(X_test)} rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot the training data points using a scatter plot with different colors for each class. \n",
    "\n",
    "**Your task:** Try to understand the code below by writing a comment at the end of each line explaining what it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mplot() \n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m target \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m      4\u001b[0m     class_name \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mtarget_names[target] \u001b[38;5;66;03m# gets the name of the class\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots() \n",
    "\n",
    "for target in range(3):\n",
    "    class_name = dataset.target_names[target] # gets the name of the class\n",
    "    ax.scatter(*X_train[y_train == target].T, label=class_name) # takes only the first two columnds of the data\n",
    "\n",
    "ax.set_xlabel(dataset.feature_names[0])\n",
    "ax.set_ylabel(dataset.feature_names[1])\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your tasks:**\n",
    "\n",
    "First, import the `KNeighborsClassifier` class from `sklearn.neighbors`.\n",
    "\n",
    "Then, for each distinct value of $k$, \n",
    "\n",
    "1. Create a classifier and fit it to the training data.\n",
    "2. Make predictions on the training data, compute the accuracy and append it to the list `training_accuracies`.\n",
    "3. Make predictions on the validation data (not used for training the classifier), compute the accuracy and append it to the list `validation_accuracies`.\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "- You can create a classifier instance `clf` as follows: `clf = KNeighborsClassifier(n_neighbors=k)`.\n",
    "- Call `clf.fit(X_train, y_train)` to fit the classifier (training).\n",
    "- Calling `clf.score(X, y)` will use the fitted classifier to predict the labels based on `X`, compare the predictions with `y` and return the accuracy (percentage of correct predictions).\n",
    "\n",
    "**Warning:** When calling `clf.score()` make sure the features `X` and the labels `y` correspond to eachother. That is, calling `clf.score(X_val, y_train)` is wrong (and might not even throw an error if they are of the same length)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO! Import the KNeighborsClassifier from sklearn.neighbors\n",
    "\n",
    "ks = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25, 30]\n",
    "validation_accuracies = []\n",
    "training_accuracies = []\n",
    "\n",
    "for k in  ks:\n",
    "    clf = ... #TODO! Create a KNeighborsClassifier with k neighbors\n",
    "    clf.fit(...) #TODO! Fit the classifier to the training data\n",
    "\n",
    "    training_accuracy = ... #TODO! Calculate the accuracy of the classifier on the training data\n",
    "    training_accuracies.append(...) #TODO! Append the accuracy to the training_accuracies list\n",
    "\n",
    "    #TODO! Do the same for the validation data as you did for the training data above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your task:** Plot both the training and validation accuracies obtained in the previous step as functions of $k$. We label the axes and use labels and a legend to make the visualization easy to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(ks, ..., label=...) #TODO! Plot the training accuracies as a function of k and label it with \"Training\"\n",
    "ax.plot(..., ..., label=...) #TODO! Plot the validation accuracies as a function of k and label it with \"Validation\"\n",
    "\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"k\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the training accuracy when $k=1$ is not $100\\%$. The reason for this is that there exist duplicate data points with different labels so the classifier has to chose one. Which one is chosen depends on the ordering of the training data when using the `sklearn` implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your task:** Use `np.argmax()` to find the value for $k$ that gave us the highest *validation* accuracy. *Not the training accuracy, because that is always $k=1$ (why?).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_idx = np.argmax(...) #TODO! Find the index of the highest validation accuracy\n",
    "best_k = ... #TODO! Find the k value that gave the highest validation accuracy by using the index found above\n",
    "best_accuracy = ... #TODO! Find the highest validation accuracy, again using the index found above\n",
    "\n",
    "print(f\"The best validation accuracy achieved was {best_accuracy * 100:.3f}% when k={best_k}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now chosen the optimal value for $k$ based on the performance on validation data and stored it in the variable `best_k`.\n",
    "\n",
    "Let us train a $k$-NN classifier with this optimal $k$ value and see how it perform on the unseen test data. \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Merge the training and validation data since we are done with using our validation data.\n",
    "2. Fit the classifier on the merged dataset using `clf.fit()`.\n",
    "3. Use `clf.score()` to compute the test accuracy.\n",
    "   \n",
    "Note that evaluating on validation data (validation accuracy) gives a very optimistic estimate of the real accuracy since we used it to decide what $k$ should be. The test data however, have not been used for making any decisions about our final model and should give us a more realistic estimate of the model's performance on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_and_val = np.concatenate(...) #TODO! Concatenate the training and validation data features\n",
    "y_train_and_val = np.concatenate(...) #TODO! Concatenate the training and validation data targets\n",
    "\n",
    "clf = ... #TODO! Create a KNeighborsClassifier with the best k value\n",
    "clf.fit(...) #TODO! Fit the classifier to the training and validation data (merged)\n",
    "test_accuracy = ... #TODO! Calculate the accuracy of the classifier on the unseen test data\n",
    "\n",
    "print(f\"Final accuracy on unseen test data: {test_accuracy * 100:.3f}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 – Plotting decision boundaries as $k$ varies\n",
    "\n",
    "In this part, we are going to visualize how the fitted $k$-NN classifier decides which class a sample belongs to.\n",
    "\n",
    "The idea is to create a uniform grid of points in the plane, classify each of these points using our classifier, and then color them based on the prediction we get.\n",
    "\n",
    "We want to make a pretty visualization, so we start by an example of how one can create custom color maps with `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "\n",
    "dark_cmap = mcolors.ListedColormap([\"#9E7702\", \"#0C235D\", \"#980026\"])\n",
    "light_cmap = mcolors.ListedColormap([\"#FFC500\", \"#4286DE\", \"#D46591\"])\n",
    "\n",
    "# Preview the colormaps\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8, 3))\n",
    "axs[0].imshow([[0, 1, 2]], cmap=dark_cmap)\n",
    "axs[0].set_title(\"Dark Colormap\")\n",
    "axs[0].axis(\"off\")\n",
    "axs[1].imshow([[0, 1, 2]], cmap=light_cmap)\n",
    "axs[1].set_title(\"Light Colormap\")\n",
    "axs[1].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So pretty!\n",
    "\n",
    "Next, we are going to write a function `plot_knn_decision_boundaries(X_train, y_train, X_val, y_val, clf, ax, resolution=0.05)` that takes in \n",
    "- Training and validation data (features) `X_train` and `X_val`,\n",
    "- corresponding labels `y_train` and `y_val`, \n",
    "- a fitted classifier instance `clf`, \n",
    "- and an axis object `ax`.\n",
    "\n",
    "**Your task:** The function is partially complete, but there is one thing you need to do. Namely, classify the points on the grid using the given classifier `clf`. See the line with the `#TODO!` comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for plotting data points together with kNN classifier's decision boundaries\n",
    "def plot_knn_decision_boundaries(X_train, y_train, X_val, y_val, clf, ax, resolution=0.05):\n",
    "    dark_cmap = mcolors.ListedColormap([\"#9E7702\", \"#0C235D\", \"#980026\"])\n",
    "    light_cmap = mcolors.ListedColormap([\"#FFC500\", \"#4286DE\", \"#D46591\"])\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - .1, X[:, 0].max() + .1\n",
    "    y_min, y_max = X[:, 1].min() - .1, X[:, 1].max() + .1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, resolution),\n",
    "                         np.arange(y_min, y_max, resolution))\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "    Z = ... #TODO! Use the classifier to predict the class of each point in the grid\n",
    "\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.pcolormesh(xx, yy, Z, cmap=light_cmap, zorder=1, norm=mcolors.NoNorm())\n",
    "\n",
    "    for target in range(3):\n",
    "        class_name = dataset.target_names[target]\n",
    "        ax.scatter(*X_train[y_train == target].T, label=f\"{class_name} (train)\", color=dark_cmap(target), edgecolor=\"black\", zorder=2, marker=\"o\")\n",
    "        ax.scatter(*X_val[y_val == target].T, label=f\"{class_name} (val)\", color=dark_cmap(target), edgecolor=\"black\", zorder=2, marker=\"s\")\n",
    "\n",
    "    ax.set_xlabel(dataset.feature_names[0])\n",
    "    ax.set_ylabel(dataset.feature_names[1])\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test our function. \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "Let `k` loop over `[1, 5, 10, 25, 50, 75, 100]`. For each iteration, \n",
    "\n",
    "1. Fit a $k$-NN classifier on the training data using `KNeighborsClassifier` with `n_neighbors=k`,\n",
    "2. Use `plot_knn_decision_boundaries` to plot the data points and the decision boundaries.\n",
    "\n",
    "Note that this code can take some seconds to finish because of the plotting function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = [1, 5, 10, 25, 50, 75, 100]\n",
    "fig, axs = plt.subplots(nrows=len(ks), ncols=1, figsize=(8, 6 * len(ks)))\n",
    "\n",
    "for k, ax in zip(ks, axs):\n",
    "    clf = ... #TODO! Create a KNeighborsClassifier with k neighbors\n",
    "    ... #TODO! Fit the classifier to the training data\n",
    "    plot_knn_decision_boundaries(...) #TODO! Plot the decision boundaries on the current axis using the function we defined above\n",
    "    ax.set_title(f\"kNN Decision Boundaries with k={k}\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two things we should think about are:\n",
    "\n",
    "**Q1:** How does the decision boundaries change as `k` increases?\n",
    "\n",
    "**Q2:** Why does the classifier never predict one of the classes when `k=100`?\n",
    "\n",
    "---\n",
    "\n",
    "As a last experiment in this part, let us fit a classifier with `k` being equal to the number of samples in the training data.\n",
    "\n",
    "**Your task:** Fit a $k$-NN classifier with `k` being equal to the number of samples in the training data. That is, use `len(y_train)` as $k$. Then, as above, plot the decision boundaries and data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "k = ... #TODO! Let k be equal to the number of training samples\n",
    "clf = ... #TODO! Create a KNeighborsClassifier with k neighbors and fit it to the training data\n",
    "\n",
    "plot_knn_decision_boundaries(X_train, y_train, X_val, y_val, clf, ax)\n",
    "ax.set_title(f\"kNN Decision Boundaries with k={k}\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** What do you observe and why does this happen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 (bonus) – Implementing your own $k$-NN classifier from scratch\n",
    "\n",
    "If you really like to test your understanding of the $k$-NN classifier, here is an extra challenge for you:\n",
    "\n",
    "1. Using only functions from NumPy and `KDTree` ([docs](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.KDTree.html)) from `scipy.spatial`, create your own $k$-NN classifier.\n",
    "2. Fit the classifier on training data (you can use the best value for $k$ that you previously found).\n",
    "3. Evaluate your classifier on the test data by computing the accuracy.\n",
    "4. Compare your test accuracy to the `sklearn` implementation.\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "- It can be useful to define a class for your $k$-NN classifier.\n",
    "- Fitting a $k$-NN classifier is simply storing the training features and labels (all the magic happens at prediction time).\n",
    "- For prediction, use `KDTree` to find the $k$ nearest neighbors of each point in `X_test`.\n",
    "- To predict, you can use `np.bincount()` together with `np.argmax()` to find the majority class among the $k$ nearest neighbors.\n",
    "- To predict without using a for-loop, you can consider using `np.apply_along_axis()`.\n",
    "- To make your classifier class compatible with `sklearn` syntax, include the following four methods in your class implementation:\n",
    "  - `__init__(self, k)` (return nothing)\n",
    "  - `fit(self, X_train, y_train)` (return nothing)\n",
    "  - `predict(self, X_test)` (return predicted labels)\n",
    "  - `score(self, X_test, y_test)` (return accuracy as float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import KDTree\n",
    "\n",
    "class KNNClassifier:\n",
    "    def __init__(self, k):\n",
    "        ... #TODO! Store the number of neighbors to consider and initialize any other variables you might need\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        ... #TODO! Store the training data features and targets in the class\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        ... #TODO! Implement the prediction logic using a KDTree and return the predictions\n",
    "    \n",
    "    def score(self, X_test, y_test):\n",
    "        ... #TODO! Implement the scoring logic using the predict function and return the accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO! Test the KNNClassifier on test data using the best k value found earlier and print the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "INF264",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
