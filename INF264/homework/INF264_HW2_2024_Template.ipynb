{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent and Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from sklearn import datasets, metrics\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Univariate linear regression with gradient descent\n",
    "\n",
    "#### Question 1\n",
    "\n",
    "You can write math in markdown cells using LaTeX. For inline math, use `$...$`. For display math, use `$$...$$`.\n",
    "\n",
    "For example, you can write the MSE loss function as\n",
    "\n",
    "$$ L(w_0, w_1) = \\frac{1}{N} \\sum_{i=0}^{N-1} \\epsilon_i(w_0, w_1)^2$$\n",
    "    \n",
    "where $\\epsilon_i(w_0, w_1) = y_i - (w_0 + w_1 x_i)$ are the residuals.\n",
    "\n",
    "`<Your Solution Here>` $\\nabla_w L =\n",
    "\\begin{bmatrix}\n",
    " -\\frac{2}{N} \\sum_{i=0}^{N-1} \\epsilon_i(w_0, w_1) \\\\\n",
    " \\\\\n",
    " -\\frac{2}{N} \\sum_{i=0}^{N-1} x_i \\epsilon_i(w_0, w_1) \\\\\n",
    "\\end{bmatrix}=-\\frac{2}{N}\\begin{bmatrix}\n",
    " \\sum_{i=0}^{N-1} \\epsilon_i(w_0, w_1) \\\\\n",
    " \\\\\n",
    " \\sum_{i=0}^{N-1} x_i \\epsilon_i(w_0, w_1) \\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "solution is a vector^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lin alg måten med regression er en loss functioin hvor man måler normalen fra expected linjen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by loading and visualizing the data from `unilinear.npy`. Use `np.load` to load the data and `plt.scatter` to visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAGdCAYAAACyzRGfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAphUlEQVR4nO3df3RU9Z3/8dfk1ySwZDAokJQEolVQRGxVclCrcBrFLEXYHhU8laW0tl3XrsuX1irnLFKO6wl0/Rbbnhzd7VGxVnEtFezaCioSWBVBA7TStRBoIEREzsaSIQykkHy+f/CdMZNMMnNn7r1zZ+b5OGeOZnJn8vlwifPy8+P98RljjAAAAFySl+4GAACA3EL4AAAAriJ8AAAAVxE+AACAqwgfAADAVYQPAADgKsIHAABwFeEDAAC4qiDdDeirp6dHR44c0bBhw+Tz+dLdHAAAkABjjE6cOKGKigrl5Q0+tuG58HHkyBFVVlamuxkAACAJhw8f1pgxYwa9xnPhY9iwYZLONb60tDTNrQEAAIkIBoOqrKyMfI4PxnPhIzzVUlpaSvgAACDDJLJkggWnAADAVYQPAADgKsIHAABwFeEDAAC4ivABAABcRfgAAACuInwAAABXET4AAICrCB8AAMBVhA8AAOAqwgcAAHAV4QMAgCzV2h7S2qY2tbaH0t2UKJ47WA4AAKSutT2kGY9t1akz3SopzNfGRTeoasSQdDdLEiMfAABkpR0HP9WpM92SpFNnurXj4KdpbtFnCB8AAGShKePKVFKYL0kqKczXlHFlaW7RZ5h2AQAgw7W2h7Tj4KeaMq4sMrVSNWKINi66od/zsa51G+EDAIAMNtjajqoRQ6IChlfWgTDtAgBABrOytsMr60AIHwAAZDArazu8sg7EZ4wxafnJAwgGgwoEAuro6FBpaWm6mwMAgOdZWcfh1JoPK5/frPkAACDD9V3bYde1TmHaBQAAuIrwAQAAXEX4AAAAriJ8AAAAVxE+AACAqwgfAABkgdb2kNY2tam1PZTupsTFVlsAADKcV8qmJ4qRDwAAMpxXyqYnivABAECG80rZ9EQx7QIAQIarGjFEGxfd4EjZdCcQPgAAyAJeKJueKKZdAACAqwgfAADAVZbDx9atWzVr1ixVVFTI5/Np/fr1/a758MMPdeuttyoQCGjo0KG65ppr1Nraakd7AQBAhrMcPk6ePKnJkyeroaEh5vcPHDig66+/XhMmTFBjY6P+8Ic/aOnSpSouLk65sQAA4JxMKirWl88YY5J+sc+ndevWac6cOZHn5s2bp8LCQj377LNJvWcwGFQgEFBHR4dKS0uTbRoAAFnLi0XFrHx+27rmo6enR7/97W91ySWXaMaMGRo5cqRqampiTs2EdXV1KRgMRj0AAMDAMq2oWF+2ho9jx46ps7NTK1as0C233KLXXntNf/d3f6evfvWr2rJlS8zX1NfXKxAIRB6VlZV2NgkAgKyTaUXF+rJ12uXIkSP63Oc+pzvvvFPPP/985Lpbb71VQ4cO1Zo1a/q9R1dXl7q6uiJfB4NBVVZWMu0CAMAgWttDnioqZmXaxdYiY+eff74KCgp02WWXRT1/6aWX6q233or5Gr/fL7/fb2czAADIeplUVKwvW6ddioqKdM0112jv3r1Rz+/bt09jx46180cBAIAMZXnko7OzU/v374983dLSot27d6usrExVVVW6//77NXfuXN1www2aPn26NmzYoP/6r/9SY2Ojne0GAAAZyvKaj8bGRk2fPr3f8wsWLNDq1aslSU899ZTq6+vV1tam8ePHa/ny5Zo9e3ZC789WWwAAMo+Vz++UFpw6gfABAEA0ry0ujSVtC04BAIC9vFhQLFUcLAcAgIdlekGxWAgfAAB4WKYXFIuFaRcAADysasQQbVx0g+fXfFhB+AAAwOMyuaBYLEy7AAAAVxE+AACAqwgfAADAVYQPAAAG0doe0tqmNrW2h9LdlKzBglMAQE5IpkpoNhb48gLCBwAg6yUbImIV+CJ8pI5pFwBA1ku2Smg2FvjyAkY+AABZLxwiwiMfiYaIbCzw5QWcagsAyAmZcDJsJuNUWwAA+si2KqGZjDUfAADAVYQPAADgKsIHAAAWUHQsdaz5AAAgQRQdswcjHwAAJCjZeiGIRvgAACBBFB2zB9MuAAAkiKJj9iB8AABgAfVCUse0CwAAcBXhAwAAuIrwAQAAXEX4AAAAriJ8AAAAVxE+AACAqwgfAADAVYQPAEBacVBb7qHIGAAgbTioLTcRPgAAadP3oLZX93ysEX/jp3R5liN8AADSJnxQ26kz3fIX5GnV6/t0+mwPoyBZjvABAEib3ge1tXd2qf7VP0n67Lh6wkd2YsEpAMBR8RaUVo0YotuuGqO6y8tdOa6eBa7px8gHAMAxVhaUunFcPQtcvYHwAQBwTN8FpfGmUpw+rt5qe+AMpl0AAI4JLyiVnJ1KydT25CqfMcakuxG9BYNBBQIBdXR0qLS0NN3NAQCkqLU95OhUSqa3J1tY+fxm2gUA4Cinp1Li6Rs20t0eED4AAFmMBabexJoPAIAnOLEFNtYCU6QfIx8AANsku57CqRGK3hVUWWDqHYQPAIAtUgkQTm2BdaN2CKwjfAAAbJFKgLBjhGKgURcWmHoP4QMAYItUAkSqIxQsLM0shA8AgC0SDRBOjFBQuTSzED4AALaJFyBYWAqJ8AEAcBELSyERPgAALnJyhIKFpZmD8AEAcA0jFJAIHwAAlzFCAcvl1bdu3apZs2apoqJCPp9P69evH/Daf/iHf5DP59Njjz2WQhMBAEA2sRw+Tp48qcmTJ6uhoWHQ69atW6d3331XFRUVSTcOAABkH8vTLnV1daqrqxv0mo8++kj/9E//pI0bN2rmzJlJNw4AAGQf20+17enp0fz583X//fdr4sSJdr89AADIcLYvOF25cqUKCgp03333JXR9V1eXurq6Il8Hg0G7mwQAOSfZ02UBN9gaPpqamvSTn/xEO3fulM/nS+g19fX1Wr58uZ3NAICcxjkn8Dpbp13++7//W8eOHVNVVZUKCgpUUFCgQ4cO6Xvf+57GjRsX8zVLlixRR0dH5HH48GE7mwQAOSdWFVHAS2wd+Zg/f75qa2ujnpsxY4bmz5+vhQsXxnyN3++X3++3sxkAkNPsqiLK1A2cYjl8dHZ2av/+/ZGvW1patHv3bpWVlamqqkojRoyIur6wsFCjR4/W+PHjU28tACAuO6qIMnUDJ1kOH++//76mT58e+Xrx4sWSpAULFmj16tW2NQwAkLxUq4hyRD2cZDl8TJs2TcaYhK8/ePCg1R8BAEgzjqiHkzjbBQDQDwfAwUm2FxkDAAAYDCMfAIB+WHAKJzHyAQDoh1ohcBLhAwDQT3jBqSQWnMJ2TLsAAPphwSmcRPgAAMSUaq0QYCBMuwAAAFcRPgAAgKsIHwAAwFWEDwAA4CrCBwB4SGt7SGub2tTaHkp3UwDHsNsFADyCqqLIFYx8AIAHtLaH1NC4n6qiyAmMfABAmvUe8QijqiiyGeEDANKs9zkqkjT3mkrdO+3zTLkgazHtAgBp1vccFYIHsh0jHwCQZpyjglxD+AAAD+AcFeQSpl0AwEXU8QAY+QAA1yRSx6O1PcT0C7Ie4QMAXNJ7V0u4jkfvgEGRMeQKpl0AwCV9d7X0reMRK5wA2YiRDwBwSbxdLeFwEh75oMgYspXPGGPS3YjegsGgAoGAOjo6VFpamu7mAICrWPOBTGXl85uRDwDwELbcIhew5gMAALiK8AEAAFxF+AAAAK4ifACAC6hsCnyGBacA4DCKhwHRGPkAAIdRPAyIRvgAAIfFq2wK5BqmXQDAYfEqmwK5hvABAC6geBjwGaZdAACAqwgfAADAVYQPAADgKsIHAABwFeEDAOKgOilgL3a7AMAgqE4K2I/wAQAxtLaHtOPgp2rv7OpXnZTwAaSG8AEg44WDgl0FvHqPdvgL8lRckKfTZ3uoTgrYhPABIKM5MS3S+yyWrrM9WlI3QSP+xk91UsAmhA8AGS3WoW2pBoTwWSzhQFN3eXnc97R79AXIZoQPABmtb1CwY1rE6lksLEoFrCF8AMhoTh3aZuUsFidGX4BsRvgAkPHSfWibE6MvQDYjfADIaXas1XBq9AXIVoQPADnr3QPtmv/Udp3pNimt1WCxKWAN4QNATmptD0WCh5T8Wg0WmwLWcbYLgJy04+CnkeAhSYX5vqTWasRabApgcIQPADkpvEhUOhc8nv1GTVIjFr3fh8WmQGJ8xhgT/zL3BINBBQIBdXR0qLS0NN3NAZDF7FqrwZoPwNrnt+WRj61bt2rWrFmqqKiQz+fT+vXrI987c+aMHnjgAU2aNElDhw5VRUWF/v7v/15Hjhyx3AkAcFrViCG67aoxKQcGu94HyBWWw8fJkyc1efJkNTQ09PteKBTSzp07tXTpUu3cuVMvvfSS9u7dq1tvvdWWxgJAWGt7SGub2tTaHkp3UwBYlNK0i8/n07p16zRnzpwBr3nvvfc0ZcoUHTp0SFVVVXHfk2kXILckM2XBDhPAe6x8fju+1bajo0M+n0/Dhw+P+f2uri51dXVFvg4Gg043CYBHJBsiKGcOZDZHd7ucPn1aDzzwgO68884BU1B9fb0CgUDkUVlZ6WSTAHhIsttU++4wGTO8hCkYIIM4NvJx5swZ3XHHHTLG6PHHHx/wuiVLlmjx4sWRr4PBIAEEyBHxzkQZaEqmdznzMcNLtHD1e0zBABnEkfARDh6HDh3Sm2++Oejcj9/vl9/vd6IZADxusDNR4k3JhA+TW9vUFncKhq2wgLfYHj7CwaO5uVmbN2/WiBEj7P4RALLIQCfSJrquI5HRExanAt5iOXx0dnZq//79ka9bWlq0e/dulZWVqby8XLfddpt27typV155Rd3d3Tp69KgkqaysTEVFRfa1HEBWS/SY+ngnyrI4FfAey1ttGxsbNX369H7PL1iwQD/84Q9VXV0d83WbN2/WtGnT4r4/W20BhNkxXcLIB+AOK5/flFcHkPVY8wE4z1N1PgAg3QZaVwIgPTjVFoCrKIsOgJEPAK5JdP0F0yRAdiN8AHBNIjtPBgooBBIgexA+ALgmke2zA5VcZ8cKkD0IHwBcE68mhxQ7oFCrA8guhA8Aroq382SggJJIwTEAmYE6HwAyAms+AG+jzgeArEOtDiB7UOcDAAC4ivABwBaJFg+jyBgApl0ApMxK8TC2zAJg5ANAygaqzZHsdQCyG+EDQMrCtTkkDboVNtHrAGQ3ttoCsEWiW2HZMgtkJ7baAnBdolth2TILgGkXAK5itwsARj4AuIbdLgAkRj4AuIjdLgAkwgcAF7HbBYDEtAsAFw10Yi2A3EL4AOAqdrsAIHwAOaB3bQ1JjDwASCvCB5Dleu8w8RfkySfp9NmepHabJFsgrLU9pFf3fCxJqru8nNAD5DjCB5Dleu8w6TrbE3k+vNsk0SCQzDbZcOj4v6/t1V+7zxVTXvX6Pr32f24kgAA5jPABZLnwDpNYIx9WdpvE2iYbr4x6OKz0dvpsj6XQAyD7ED6ADJbINEjfHSZScms+eoeYRIJL77DSW3FBHltsgRxH+AAy1EDTILECSd8dJsmMOljdJtt3xGXhdeN03pAi1nwAIHwAmWqgaqFOli+3sk2Wmh4ABkKFUyBDxaoW2jeQhHeYpEvViCG67aoxBA8AURj5ADLUQCML/oK8yK6WVa/vY5oDgOcw8gFksL4jC1UjhmjxTZdEvh/eWQIAXkL4ALJM3eXlHN4GwNOYdgGyDAs9AXgd4QPIQhzeBsDLmHYBXNbaHtLapja1tofS3RQASAtGPgAXJXM+CgBkG0Y+ABcNVBjMTYy8AEg3Rj4AF1k9H8VujLwA8ALCB+CidO9EsXoyLQA4gfABuCzRnSiJnFhrVbpHXgBAInwAnuTU9Ei6R14AQCJ8AJ7k5PRI+H12HPxUR46fUtvxUwQRAK4ifAAOS2b6xMnpkd6jKmEsPgXgJsIH4KBkp0+SnR5JJOj0HlUJY/EpADcRPgAHpTJ9YrVEeqJBp/eoShiLTwG4ifABOMjN3SWDBZ2+IyLhUZUxw0tY8wHAdYQPwEFu7i4ZKOgMNCJC2ACQLoQPwGFufdAPFHQoLAbAawgfQBaJFXQoLAbAawgfQJajsBgAryF8ADmANR4AvCQv3Q0AAAC5hfABeFRre0hrm9rU2h5Kd1MAwFaWw8fWrVs1a9YsVVRUyOfzaf369VHfN8booYceUnl5uUpKSlRbW6vm5ma72gtkhFSDQ3h77Pd/9XvNeGwrAQRAVrEcPk6ePKnJkyeroaEh5vd/9KMf6ac//ameeOIJbd++XUOHDtWMGTN0+vTplBsLZAI7gkOs7bEAkC0sLzitq6tTXV1dzO8ZY/TYY4/pX/7lXzR79mxJ0i9+8QuNGjVK69ev17x581JrLZAB7KirMWZ4yaBfA0Ams3XNR0tLi44ePara2trIc4FAQDU1Ndq2bVvM13R1dSkYDEY9gEw2ZVyZigvO/WoVF+QlVVej7fipQb8GgExma/g4evSoJGnUqFFRz48aNSryvb7q6+sVCAQij8rKSjubBKSF6fNPq8KFwSQOfQOQfdK+22XJkiXq6OiIPA4fPpzuJgEp2XHwU3Wd7ZEkdZ3tSWq9Rrgw2KO3Tx7wdFoAyFS2FhkbPXq0JOmTTz5ReXl55PlPPvlEV155ZczX+P1++f1+O5sBpJVd5cwpDAYgW9k68lFdXa3Ro0dr06ZNkeeCwaC2b9+uqVOn2vmjAM9i1AIABmd55KOzs1P79++PfN3S0qLdu3errKxMVVVVWrRokf71X/9VF198saqrq7V06VJVVFRozpw5drYb8LRkRi1a20OcvwIgJ1gOH++//76mT58e+Xrx4sWSpAULFmj16tX6wQ9+oJMnT+rb3/62jh8/ruuvv14bNmxQcXGxfa0Gsky4Nkh4qoYREwDZzGeMSXZBviOCwaACgYA6OjpUWlqa7uYArljb1Kbv/+r3ka8fvX2ybrtqTBpbBADWWPn8TvtuFwBsrQWQW2zd7QLkqlTXa4QXqbLmA0AuIHwAKbJrvQZbawHkCqZdgBRxCBwAWEP4AFLEeg0AsIZpFyBFrNcAAGsIH4ANWK8BAIkjfAAJ6L2bRRKjHACQAsIHEEfv3Sz+gjz5JJ0+20MlUgBIEgtOgTh672bpOtuj02d7JLGzBQCSRfgA4ui9m8VfkKfignO/NuxsAYDkMO0CxNF3N4vEmg8ASAXhA0hA390shA4ASB7TLsh4re0hrW1qU2t7KN1NAQAkgJEPZDS7zlUBALiHkQ9kNM5VAYDMQ/hARrN6rkoiUzRM4wCAs5h2QUazcq5KIlM0TOMAgPMIH8h4iZ6r0neK5tU9H2vE3/ijQkusaRzCBwDYi/CBnBGeogmXSV/1+r5+ZdJ7X0MRMQBwBuEDOaP3FE17Z5fqX/2TpOgRDivTOACA5BA+kFPCAaO1PaTH3miOOcKR6DQOACA5hA/kJEY4ACB9CB/IWYxwAEB6UOcDAAC4ivABAABcRfgAAACuInwAAABXET6QUTh3BQAyH7td4KjW9pBt21k5dwUAsgPhA46xOyxw7goAZAemXeCYWGEhFeFzVyRx7goAZDBGPuAYuw9poyopAGQHnzHGpLsRvQWDQQUCAXV0dKi0tDTdzUGK7FzzAQDwLiuf34x8wFFOlzAn3ABA5iF8IGOx+wUAMhMLTpGx7F7QCgBwB+EDtrG7AFi892P3CwBkJqZdYAu7p0ASeT92vwBAZiJ8wBZ2FwBL9P2cXtAKALAf0y6wxZjhJSrM90myZwqEKRUAyF6MfCBlre0hLVz9ns50GxXm+/T0169JeTSCKRUAyF6EjxyXaJ2Mwa7rPUVyptuo7fipQV8ffk28n8mUCgBkJ8JHDkt0kWi86+KVUe/9en9BnnySTp/toTYHAOQowkcOS3RRZ7zr4k2R9H5919meyPOcTAsAuYnwkcMSPfgtkesGmyLp/fq+Ix8sJAWA3MPBcjku0bUYqZ6hksyaDwBA5rDy+U34gCTOSQEApMbK5zd1PiCJc1IAAO4hfEASRb0AAO5hwSkkUdQLAOAewgciKOoFAHAD0y4AAMBVhA8AAOAq28NHd3e3li5dqurqapWUlOiiiy7Sww8/LI/t6AUAAGli+5qPlStX6vHHH9czzzyjiRMn6v3339fChQsVCAR033332f3jAABAhrE9fLzzzjuaPXu2Zs6cKUkaN26c1qxZox07dtj9owAAQAayfdrl2muv1aZNm7Rv3z5J0u9//3u99dZbqquri3l9V1eXgsFg1APnKo6ubWpTa3so3U0BAMBWto98PPjggwoGg5owYYLy8/PV3d2tRx55RF/72tdiXl9fX6/ly5fb3YyMlmyp81TPXwEAwA22h48XX3xRzz33nJ5//nlNnDhRu3fv1qJFi1RRUaEFCxb0u37JkiVavHhx5OtgMKjKykq7m5VREj3qvjfOZgEAZArbw8f999+vBx98UPPmzZMkTZo0SYcOHVJ9fX3M8OH3++X3++1uRkZL9Kj73pIJLH0xcgIAcIPt4SMUCikvL3opSX5+vnp6euz+UVkrmVLnyQSW3hg5AQC4xfbwMWvWLD3yyCOqqqrSxIkTtWvXLv34xz/WN77xDbt/VFZLpNR535GKVM5msWPkBACARNgePn72s59p6dKl+sd//EcdO3ZMFRUV+s53vqOHHnrI7h+V0wYaqUg2MKQ6cgIAQKJ8xmOlR4PBoAKBgDo6OlRaWpru5njW2qY2ff9Xv498/ejtk3XbVWNSek/WfAAAkmXl85tTbTOUEyMVnGoLAHAD4SNDpbrGAwCAdCF8ZDBGKgAAmcj28uoAAACDIXwAAABXET5sxoFwAAAMjjUfNnKqSihbYAEA2YTwYSMnqoRS9hwAkG2YdrFRuPaGJNtqb8QKNAAAZDJGPmzkRO0Nyp4DALIN5dUzAGs+AABeR3l1D0olQFBMDACQTQgfLmDRKAAAn2HBqQtYNAoAwGcIHy5wYhcMAACZimkXF3ACLQAAnyF8uGSwRaPsZgEA5BLCRxxOBwMWowIAcg3hYxB2B4NYQcaJkuwAAHgZ4WMQdgaDgYIMFUwBALmG8KGBp1bsDAYDBRkWowIAck3Oh4/BplbsDAaDBRkqmAIAcknOh494Uyt2BQNGOAAAOCfnw0fvEYnCfJ/GDC9x7GcxwgEAABVOVTViiJ7++jUqzPfpTLfRwtXvqbU91O+61vaQ1ja1xfweAABIXM6PfEhS2/FTOtNtJMWeeqEWBwAA9sn5kQ8p/tkrHAwHAIB9cmrkY6AttfEWg1KLAwAA+/iMMSbdjegtGAwqEAioo6NDpaWltr1vqlMniZRZ54wWAECusvL5nTMjH6lWK423U4V1IQAAJCZn1nzEW9eRKtaFAACQmJwZ+XC6yBfrQgAASEzOrPlwA2s+AAC5ijUfaUIFUwAA4suZNR8AAMAbCB8AAMBVhA8AAOAqwgcAAHAV4QMAALiK8AEAAFxF+AAAAK4ifAAAAFcRPgAAgKsIHzpXFn1tU5ta20PpbgoAAFkv58urt7aHNOOxrZED4TYuuoES6QAAOCjnRz52HPxUp850S5JOnenWjoOfprlFAABkt5wPH1PGlamkMF+SVFKYrynjytLcIgAAslvOT7tUjRiijYtu0I6Dn2rKuDKmXAAAcFjOhw/pXAAhdAAA4I6cn3YBAADuInwAAABXET4AAICrCB8AAMBVhA8AAOAqR8LHRx99pLvuuksjRoxQSUmJJk2apPfff9+JHwUAADKM7Vtt//KXv+i6667T9OnT9eqrr+qCCy5Qc3OzzjvvPLt/FAAAyEC2h4+VK1eqsrJSTz/9dOS56upqu38MAADIULZPu/zmN7/R1Vdfrdtvv10jR47UF77wBf385z8f8Pquri4Fg8GoBwAAyF62h48///nPevzxx3XxxRdr48aNuueee3TffffpmWeeiXl9fX29AoFA5FFZWWl3kwAAgIf4jDHGzjcsKirS1VdfrXfeeSfy3H333af33ntP27Zt63d9V1eXurq6Il8Hg0FVVlaqo6NDpaWldjYNAAA4JBgMKhAIJPT5bfvIR3l5uS677LKo5y699FK1trbGvN7v96u0tDTqAQAAspftC06vu+467d27N+q5ffv2aezYsQm9PjwQw9oPAAAyR/hzO6EJFWOzHTt2mIKCAvPII4+Y5uZm89xzz5khQ4aYX/7ylwm9/vDhw0YSDx48ePDgwSMDH4cPH477WW/7mg9JeuWVV7RkyRI1Nzerurpaixcv1re+9a2EXtvT06MjR45o2LBh8vl8Sf388LqRw4cPZ+00Ti70UcqNfuZCH6Xc6Gcu9FHKjX7mQh8le/tpjNGJEydUUVGhvLzBV3XYPu0iSV/5ylf0la98JanX5uXlacyYMba0IxfWkORCH6Xc6Gcu9FHKjX7mQh+l3OhnLvRRsq+fgUAgoes42wUAALiK8AEAAFyVleHD7/dr2bJl8vv96W6KY3Khj1Ju9DMX+ijlRj9zoY9SbvQzF/oopa+fjiw4BQAAGEhWjnwAAADvInwAAABXET4AAICrCB8AAMBVGRM+GhoaNG7cOBUXF6umpkY7duwY9Ppf/epXmjBhgoqLizVp0iT97ne/i/q+MUYPPfSQysvLVVJSotraWjU3NzvZhbis9PHnP/+5vvSlL+m8887Teeedp9ra2n7Xf/3rX5fP54t63HLLLU53Iy4r/Vy9enW/PhQXF0ddk+n3ctq0af366PP5NHPmzMg1XruXW7du1axZs1RRUSGfz6f169fHfU1jY6O++MUvyu/36/Of/7xWr17d7xqrv+dOs9rPl156STfddJMuuOAClZaWaurUqdq4cWPUNT/84Q/73csJEyY42IvBWe1jY2NjzL+vR48ejbrOS/fSah9j/b75fD5NnDgxco3X7mN9fb2uueYaDRs2TCNHjtScOXP6nbMWS7o+KzMifPznf/6nFi9erGXLlmnnzp2aPHmyZsyYoWPHjsW8/p133tGdd96pb37zm9q1a5fmzJmjOXPmaM+ePZFrfvSjH+mnP/2pnnjiCW3fvl1Dhw7VjBkzdPr0abe6FcVqHxsbG3XnnXdq8+bN2rZtmyorK3XzzTfro48+irrulltu0ccffxx5rFmzxo3uDMhqP6Vzlfd69+HQoUNR38/0e/nSSy9F9W/Pnj3Kz8/X7bffHnWdl+7lyZMnNXnyZDU0NCR0fUtLi2bOnKnp06dr9+7dWrRoke6+++6oD+Zk/m44zWo/t27dqptuukm/+93v1NTUpOnTp2vWrFnatWtX1HUTJ06MupdvvfWWE81PiNU+hu3duzeqDyNHjox8z2v30moff/KTn0T17fDhwyorK+v3O+ml+7hlyxbde++9evfdd/X666/rzJkzuvnmm3Xy5MkBX5PWz8pkD5Bz05QpU8y9994b+bq7u9tUVFSY+vr6mNffcccdZubMmVHP1dTUmO985zvGGGN6enrM6NGjzb/9279Fvn/8+HHj9/vNmjVrHOhBfFb72NfZs2fNsGHDzDPPPBN5bsGCBWb27Nl2NzUlVvv59NNPm0AgMOD7ZeO9XLVqlRk2bJjp7OyMPOfFexkmyaxbt27Qa37wgx+YiRMnRj03d+5cM2PGjMjXqf65OS2RfsZy2WWXmeXLl0e+XrZsmZk8ebJ9DbNRIn3cvHmzkWT+8pe/DHiNl+9lMvdx3bp1xufzmYMHD0ae8/J9NMaYY8eOGUlmy5YtA16Tzs9Kz498/PWvf1VTU5Nqa2sjz+Xl5am2tlbbtm2L+Zpt27ZFXS9JM2bMiFzf0tKio0ePRl0TCARUU1Mz4Hs6KZk+9hUKhXTmzBmVlZVFPd/Y2KiRI0dq/Pjxuueee9Te3m5r261Itp+dnZ0aO3asKisrNXv2bP3xj3+MfC8b7+WTTz6pefPmaejQoVHPe+leWhXvd9KOPzcv6unp0YkTJ/r9XjY3N6uiokIXXnihvva1r6m1tTVNLUzelVdeqfLyct100016++23I89n47188sknVVtbq7Fjx0Y97+X72NHRIUn9/u71ls7PSs+Hj//93/9Vd3e3Ro0aFfX8qFGj+s0xhh09enTQ68P/tPKeTkqmj3098MADqqioiPpLcsstt+gXv/iFNm3apJUrV2rLli2qq6tTd3e3re1PVDL9HD9+vJ566im9/PLL+uUvf6menh5de+21amtrk5R993LHjh3as2eP7r777qjnvXYvrRrodzIYDOrUqVO2/A540aOPPqrOzk7dcccdkedqamq0evVqbdiwQY8//rhaWlr0pS99SSdOnEhjSxNXXl6uJ554Qr/+9a/161//WpWVlZo2bZp27twpyZ7/nnnJkSNH9Oqrr/b7nfTyfezp6dGiRYt03XXX6fLLLx/wunR+Vjpyqi3ctWLFCr3wwgtqbGyMWow5b968yL9PmjRJV1xxhS666CI1Njbqy1/+cjqaatnUqVM1derUyNfXXnutLr30Uv37v/+7Hn744TS2zBlPPvmkJk2apClTpkQ9nw33Mtc8//zzWr58uV5++eWo9RB1dXWRf7/iiitUU1OjsWPH6sUXX9Q3v/nNdDTVkvHjx2v8+PGRr6+99lodOHBAq1at0rPPPpvGljnjmWee0fDhwzVnzpyo5718H++9917t2bMnrWtQ4vH8yMf555+v/Px8ffLJJ1HPf/LJJxo9enTM14wePXrQ68P/tPKeTkqmj2GPPvqoVqxYoddee01XXHHFoNdeeOGFOv/887V///6U25yMVPoZVlhYqC984QuRPmTTvTx58qReeOGFhP7Dle57adVAv5OlpaUqKSmx5e+Gl7zwwgu6++679eKLL/Yb1u5r+PDhuuSSSzLmXsYyZcqUSPuz6V4aY/TUU09p/vz5KioqGvRar9zH7373u3rllVe0efNmjRkzZtBr0/lZ6fnwUVRUpKuuukqbNm2KPNfT06NNmzZF/R9xb1OnTo26XpJef/31yPXV1dUaPXp01DXBYFDbt28f8D2dlEwfpXOrkB9++GFt2LBBV199ddyf09bWpvb2dpWXl9vSbquS7Wdv3d3d+uCDDyJ9yJZ7KZ3b8tbV1aW77ror7s9J9720Kt7vpB1/N7xizZo1WrhwodasWRO1XXognZ2dOnDgQMbcy1h2794daX823cstW7Zo//79Cf0PQbrvozFG3/3ud7Vu3Tq9+eabqq6ujvuatH5WprRc1SUvvPCC8fv9ZvXq1eZ//ud/zLe//W0zfPhwc/ToUWOMMfPnzzcPPvhg5Pq3337bFBQUmEcffdR8+OGHZtmyZaawsNB88MEHkWtWrFhhhg8fbl5++WXzhz/8wcyePdtUV1ebU6dOud4/Y6z3ccWKFaaoqMisXbvWfPzxx5HHiRMnjDHGnDhxwnz/+98327ZtMy0tLeaNN94wX/ziF83FF19sTp8+nZY+GmO9n8uXLzcbN240Bw4cME1NTWbevHmmuLjY/PGPf4xck+n3Muz66683c+fO7fe8F+/liRMnzK5du8yuXbuMJPPjH//Y7Nq1yxw6dMgYY8yDDz5o5s+fH7n+z3/+sxkyZIi5//77zYcffmgaGhpMfn6+2bBhQ+SaeH9u6WC1n88995wpKCgwDQ0NUb+Xx48fj1zzve99zzQ2NpqWlhbz9ttvm9raWnP++eebY8eOud4/Y6z3cdWqVWb9+vWmubnZfPDBB+af//mfTV5ennnjjTci13jtXlrtY9hdd91lampqYr6n1+7jPffcYwKBgGlsbIz6uxcKhSLXeOmzMiPChzHG/OxnPzNVVVWmqKjITJkyxbz77ruR7914441mwYIFUde/+OKL5pJLLjFFRUVm4sSJ5re//W3U93t6eszSpUvNqFGjjN/vN1/+8pfN3r173ejKgKz0cezYsUZSv8eyZcuMMcaEQiFz8803mwsuuMAUFhaasWPHmm9961tp/Q95mJV+Llq0KHLtqFGjzN/+7d+anTt3Rr1fpt9LY4z505/+ZCSZ1157rd97efFehrdb9n2E+7VgwQJz44039nvNlVdeaYqKisyFF15onn766X7vO9ifWzpY7eeNN9446PXGnNtiXF5eboqKisznPvc5M3fuXLN//353O9aL1T6uXLnSXHTRRaa4uNiUlZWZadOmmTfffLPf+3rpXibz9/X48eOmpKTE/Md//EfM9/TafYzVP0lRv2de+qz0/f9GAwAAuMLzaz4AAEB2IXwAAABXET4AAICrCB8AAMBVhA8AAOAqwgcAAHAV4QMAALiK8AEAAFxF+AAAAK4ifAAAAFcRPgAAgKsIHwAAwFX/DyTheucqh2ahAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read the x and y values from unilinear.csv\n",
    "datapoints = np.load('unilinear.npy')\n",
    "X, Y = datapoints[:,0], datapoints[:,1]\n",
    "\n",
    "plt.scatter(X,Y, 3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_loss_function(X: np.ndarray, Y: np.ndarray, w: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns the gradient of the MSE loss function for the given data points and weights\n",
    "    \"\"\"\n",
    "    N = len(X)\n",
    "    w_0 = -2/N * sum([        Y[i] - w[0] - w[1]*X[i]  for i in N])\n",
    "    w_1 = -2/N * sum([X[i] * (Y[i] - w[0] - w[1]*X[i]) for i in N])\n",
    "\n",
    "    return np.array([w_0, w_1])\n",
    "\n",
    "    \n",
    "def gradient_descent(\n",
    "    X: np.ndarray, \n",
    "    Y: np.ndarray,\n",
    "    weights: np.ndarray=np.array([0, 0]),\n",
    "    learning_rate: float=0.1,\n",
    "    iterations: int=1000,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Iteratively updates the weights using the steepest descent method\n",
    "    \"\"\"\n",
    "\n",
    "    ... #TODO!\n",
    "\n",
    "    return weights\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3\n",
    "We test our gradient descent implementation iterating $40$ times with a learning rate of $\\eta=0.1$. In addition, we plot the fitted line together with the original datapoints to visually comfirm that our implementation works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run gradient descent and print the found weights\n",
    "... #TODO!\n",
    "\n",
    "# Plot data points and the fitted linear function\n",
    "... #TODO!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4\n",
    "Compute the closed form solution $X^+Y=(X^TX)^{-1}X^TY$ using NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closed_form_solution(X: np.ndarray, Y: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the closed form solution of the loss minimization pb\n",
    "    \"\"\"\n",
    "\n",
    "    ... #TODO!\n",
    "\n",
    "    return solution\n",
    "\n",
    "weights_exp = closed_form_solution(X, Y)\n",
    "\n",
    "print(f\"The weights found using the closed form solution are: {weights_exp}\")\n",
    "\n",
    "weights = gradient_descent(X, Y, learning_rate=0.1, iterations=40)\n",
    "print(f\"The absolute differences between the weights using the two methods are {np.abs(weights - weights_exp)}\")\n",
    "\n",
    "# Try gradient descent again but with 1000 iterations instead of 40\n",
    "... #TODO!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5\n",
    "Let us now perform gradient descent using different values for the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [10**(-i) for i in range(0, 6)]\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    ... #TODO!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6\n",
    "\n",
    "Let us first define the MSE loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(X: np.ndarray, Y: np.ndarray, weights: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Returns the mean squared error loss for the given data points and weights\n",
    "    \"\"\"\n",
    "    \n",
    "    ... #TODO!\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now plot the loss function $L$ as a function of the weights $w_0$ and $w_1$ using a contour plot. We also plot the trajectory of the weights during the gradient descent optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a contour plot of the loss function\n",
    "# Create a grid of weights\n",
    "w0_values = np.linspace(-20, 20, 100)\n",
    "w1_values = np.linspace(-20, 20, 100)\n",
    "W0, W1 = np.meshgrid(w0_values, w1_values)\n",
    "\n",
    "# Compute the loss for each pair of weights\n",
    "loss = np.zeros_like(W0)\n",
    "for i in range(W0.shape[0]):\n",
    "    for j in range(W0.shape[1]):\n",
    "        loss[i, j] = mse_loss(X, Y, np.array([W0[i, j], W1[i, j]]))\n",
    "\n",
    "# Plot the loss function\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_title(\"MSE loss\")\n",
    "ax.set_xlabel(\"w0\")\n",
    "ax.set_ylabel(\"w1\")\n",
    "contour = ax.contourf(W0, W1, loss, levels=500, cmap=\"coolwarm\")\n",
    "fig.colorbar(contour)\n",
    "\n",
    "# Plot the path of the gradient descent algorithm onto ax using ax.plot\n",
    "# Experiment with different learning rates and initial weights\n",
    "learning_rate = ... #TODO!\n",
    "weights = ... #TODO!\n",
    "n_iter = ... #TODO!\n",
    "\n",
    "for i in range(n_iter):\n",
    "    ... #TODO!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Logistic Regression\n",
    "\n",
    "### 2.1 Logistic regression on two classes\n",
    "\n",
    "#### Question 1\n",
    "\n",
    "We load the Iris dataset using the built-in function from `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "print(f\"Features: {iris.feature_names}\")\n",
    "print(f\"Labels: {iris.target_names}\")\n",
    "\n",
    "# Extract the feature petal width (index 3)\n",
    "X = ... #TODO!\n",
    "\n",
    "# Convert labels to binary labels with 1 for Virginica and 0 for others (Setosa and Versicolor)\n",
    "y = ... #TODO!\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now split the dataset into training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix random seed for reproducibility\n",
    "seed = 0                  \n",
    "# Shuffle and split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = ... #TODO!\n",
    "\n",
    "print(f\"Train set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2\n",
    "\n",
    "Fitting a linear regression model using `sklearn`'s `LinearRegression` class. Use the `fit` method to fit the model to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression = ... #TODO!\n",
    "\n",
    "# Fit the model\n",
    "... #TODO!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3\n",
    "\n",
    "Fitting a logistic regression model using the `LogisticRegression` class. Again, use the `fit` method to fit the model to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = ... #TODO!\n",
    "\n",
    "# Fit the model\n",
    "... #TODO!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4\n",
    "\n",
    "We can now plot the datapoints and the probabilities predicted by the two models for different values of `petal width`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data points\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "cmap = mcolors.ListedColormap([\"#9E7702\", \"#0C235D\"])\n",
    "ax.scatter(X, y, c=y, cmap=cmap, label='Class color', alpha=0.2)\n",
    "\n",
    "# Plot linear regression predictions (use linear_regression.predict to get the predicted values)\n",
    "X_grid = np.linspace(0, 3, 500).reshape(-1, 1)\n",
    "... #TODO!\n",
    "\n",
    "# Plot Logistic regression predictions (use logit.predict_proba instead of logit.predict to get probabilities)\n",
    "... #TODO!\n",
    "\n",
    "ax.set_title(\"Logistic regression\")\n",
    "ax.set_xlabel(\"Petal width\")\n",
    "ax.set_ylabel(\"Predicted probability of being of class Iris-Virginica\")\n",
    "\n",
    "# Horizontal line at y=0.5\n",
    "ax.axhline(0.5, color=\"red\", linestyle=\"--\", label=\"Threshold\")\n",
    "\n",
    "ax.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the linear regression model predicts values outside of the interval $[0,1]$ which does not makes sense when talking about probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5\n",
    "\n",
    "Evaluate the two models on the test data. We set the cutoff value to $0.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the test accuracy for the linear and logistic regression models\n",
    "y_pred_linear = (linear_regression.predict(X_test.reshape(-1, 1)) > 0.5)\n",
    "test_accuracy_linear = metrics.accuracy_score(y_test, y_pred_linear)\n",
    "\n",
    "y_pred_logit =  ... #TODO!\n",
    "test_accuracy_logit = ... #TODO!\n",
    "\n",
    "# Print the test accuracies\n",
    " ... #TODO!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Multi class logistic regression\n",
    "\n",
    "#### Question 1\n",
    "\n",
    "We will now use two features, namely `petal width (cm)` and  `petal length (cm)`, and keep all three labels as they appear in the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the iris dataset\n",
    "iris = ... #TODO!\n",
    "\n",
    "# Extract the features petal length and petal width\n",
    "X = ... #TODO!\n",
    "\n",
    "# Use targets 0, 1 and 2 as is\n",
    "y = ... #TODO!\n",
    "\n",
    "# Print the shapes of X and y\n",
    "... #TODO!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into training, validation and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix random seed for reproducibility\n",
    "seed = 0\n",
    "# Shuffle and split the data\n",
    "X_train, X_val_test, y_train, y_val_test = ... #TODO!\n",
    "X_val, X_test, y_val, y_test = ... #TODO!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2\n",
    "\n",
    "Fitting a multi-class logistic regression model is just as easy as fitting a binary one.\n",
    "\n",
    "Note that the `LogisticRegression` class automatically detects that we have more than two classes and selects the appropriate loss function. The use of the keyword argument `multi_class` is deprecated in newer versions of `sklearn` but you will still see it in many examples online."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = ... #TODO!\n",
    "\n",
    "# Fit the model\n",
    "... #TODO!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3\n",
    "\n",
    "We plot the datapoints together with the decision boundaries of our multi-class logistic classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "dark_cmap = mcolors.ListedColormap([\"#9E7702\", \"#0C235D\", \"#980026\"])\n",
    "light_cmap = mcolors.ListedColormap([\"#FFC500\", \"#4286DE\", \"#D46591\"])\n",
    "\n",
    "x_min, x_max = X[:, 0].min() - .1, X[:, 0].max() + .1\n",
    "y_min, y_max = X[:, 1].min() - .1, X[:, 1].max() + .1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "# Predict the class for each grid point using logit.predict() on the grid\n",
    "Z = ... #TODO!\n",
    "\n",
    "Z = Z.reshape(xx.shape)\n",
    "ax.pcolormesh(xx, yy, Z, cmap=light_cmap, zorder=1, vmin=0, vmax=3)\n",
    "\n",
    "for target in range(3):\n",
    "    class_name = iris.target_names[target]\n",
    "    ax.scatter(*X_train[y_train == target].T, label=f\"{class_name} (train)\", color=dark_cmap(target), edgecolor=\"black\", zorder=2, marker=\"o\")\n",
    "    ax.scatter(*X_val[y_val == target].T, label=f\"{class_name} (val)\", color=dark_cmap(target), edgecolor=\"black\", zorder=2, marker=\"s\")\n",
    "\n",
    "ax.set_xlabel(iris.feature_names[2])\n",
    "ax.set_ylabel(iris.feature_names[3])\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4\n",
    "\n",
    "Evaluating on the test dataset by calling the `predict` method of the `LogisticRegression` class and computing the accuracy. Or we can use the `score` method which does this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "y_pred = ... #TODO!\n",
    "test_accuracy = ... #TODO!\n",
    "\n",
    "# Print the test accuracy\n",
    "... #TODO!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "71c2cb666ff353b4e7b5c350d66179fa0af5c84ce239ad9fa105d94543f3ad59"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
