{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### info264 - notater"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lecture 1 - 21/08/24\n",
    "\n",
    "\n",
    "#### GOFAI\n",
    "- needs domain\n",
    "- needs rules\n",
    "- what and how"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three important concepts\n",
    "- Task\n",
    "- Preformance measure\n",
    "- Experience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised learning\n",
    "- clustering\n",
    "- dimension reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### test\n",
    "nr.\n",
    "a g h\n",
    "x f d b\n",
    "e\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcment learning\n",
    "- no object/ label pairs, only feedback from the actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lecture 2 22/08/24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- model - is a solution to a task\n",
    "- model family (or the hypothesis) is a set of models from which the model is chosen\n",
    "\n",
    "\n",
    "types of supervised?\n",
    "- regression - continious\n",
    "- classification - discrete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mitchell's definition\n",
    "- Task\n",
    "- Perform measure\n",
    "- experience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inductive bias of learning alorithm is the set of assumptions that the learner uses to predict output given inputs that it has not encountered (Mitchel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train\n",
    "validate\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-nearest neighbor\n",
    "\n",
    "low k will make it more complex/overfit\n",
    "- potential issues:\n",
    "  - noice\n",
    "  - too little data\n",
    "  - too flexible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lecture 4\n",
    "\n",
    "univariate regression\n",
    "- univariate means that the object only has one feature\n",
    "- x: feature\n",
    "- y: label\n",
    "- $\\epsilon$: error/residual\n",
    "- y = w*x + b + $\\epsilon$\n",
    "- minimize loss on training data\n",
    "\n",
    "What is a loss function?\n",
    "- L(y, f(x)) = (y - f(x))^2\n",
    "- Minimize the sum of squared errors on the training set\n",
    "\n",
    "- Estimates for the parameters can be found by solving $\\hat{w}, \\hat{b} = arg min E(w, b)$\n",
    "- MSE\n",
    "\n",
    "Linear Models for classification\n",
    "- Logical regression\n",
    "\n",
    "Logical Regression\n",
    "\n",
    "\n",
    "lin reg + gaussian\n",
    "\n",
    "\n",
    "just add constant in\n",
    "y=W^T x  + b\n",
    "add b in with 1 in the x vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lecture 5 - decision tree\n",
    "\n",
    "- K-NN: R^n\n",
    "- Linreg: r^n\n",
    "- Linreg class: r^n\n",
    "- decision: discrete/categorial\n",
    "\n",
    "Expressiveness\n",
    "- Approximation\n",
    "- Can be large\n",
    "- Probably does not genrerlize well\n",
    "- Finding optimal decision tree - nphard\n",
    "\n",
    "Learning\n",
    "- split on a group feature\n",
    "\n",
    "heuristics\n",
    "- \"pureness\" of the featues\n",
    "\n",
    "entropy\n",
    "- measures average info obtained from a stochastic information source\n",
    "\n",
    "conditional entropy\n",
    "- Amount of information needed to describe the outcome of a random variable y when the value of a ranodm variable x is known\n",
    "\n",
    "information gain\n",
    "\n",
    "Iterative Dichotomizer 3 - ID3\n",
    "- If all data points have the same label\n",
    "  - return a leaf with the label\n",
    "- else if all points have identical features values\n",
    "  - return a leaf with the most common label\n",
    "- else\n",
    "  - something\n",
    "\n",
    "Typically overfit\n",
    "- unless something is done\n",
    "\n",
    "- Continous variables\n",
    "  - inf splits\n",
    "  - just dont\n",
    "    - mean f.ex\n",
    "\n",
    "- 100% acc\n",
    "- prefer smaller trees\n",
    "  - max depth\n",
    "- early stopping\n",
    "  - sop recursion once info gain is non-positive\n",
    "- build (post prining)\n",
    "  - prune after making the whole tree\n",
    "\n",
    "Reduced-error pruning\n",
    "  - something\n",
    "\n",
    "Regression trees\n",
    "- prediction values y using the points associated with the leave\n",
    "  - typically by averaging\n",
    "- learning\n",
    "  - using mse\n",
    "  - information gain form a split is the mse - mse beforehand\n",
    "\n",
    "Properties of decision trees\n",
    "- Pros\n",
    "  - Fast\n",
    "  - Easy to interpret\n",
    "  - Invariant to scaling\n",
    "  - both categorial and contiuous\n",
    "  - implicit features selection\n",
    "- cons\n",
    "  - unstable\n",
    "    - due to greedy learning, small differences in traning data can result totally different trees\n",
    "  - usually competitive accurcy only in an esamble\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lecture 6\n",
    "\n",
    "Model selection:\n",
    "- estimating the performance of several models in order to choose the best one\n",
    "  - model selection is a part of training\n",
    "  - select by estimating prediction error or unseen data points\n",
    "  - using statistics\n",
    "    - aic, bic\n",
    "    - heteroskedacicity\n",
    "  - validation data\n",
    "Eval\n",
    "    - eval the final models prediction error on new data\n",
    "\n",
    "Cross validation\n",
    "- k-fold cross-validation\n",
    "  - partion data ino k equally sized subset\n",
    "  - For each subset $D_i$\n",
    "    - train a model using all subsets except $D_i$\n",
    "    - Validate on validation data $D_i$\n",
    "  - Combine preformance\n",
    "  - reducing variability\n",
    "  - getting to know its variance\n",
    "\n",
    "\n",
    "Hyperparameters and cross-validation\n",
    "- Model type - param    - hyperparam\n",
    "- KNN        - trainset - k\n",
    "- lin reg    - a, b     - a , b, rate\n",
    "\n",
    "- Hyperparameters are parameters and options that you set before running a learning algorithm\n",
    "\n",
    "- Typically, it is not possible to test possible value\n",
    "  - gird search\n",
    "  - randomized search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap\n",
    "- randomly sample a new training set\n",
    "validation is training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# common performance measures\n",
    "Regression\n",
    "- RMSE\n",
    "  - root mean squared error\n",
    "    - sensitive to outliers\n",
    "- MAE\n",
    "  - mean absolute error\n",
    "Classification\n",
    "  - Accuracy\n",
    "  - Precicion and recall\n",
    "  - F1\n",
    "\n",
    "Confusion matrix\n",
    "- Matrix which contains count of false pos, true pos, and so on\n",
    "- Precision: TP/(TP + FP) \n",
    "- Recall(sensitivity): TP/(TP + FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.700439718141092\n",
      "5.309196154511837\n",
      "Information gain1: 0.3912435636292546\n",
      "Information gain2: 0.8112781244591325\n",
      "Information gain3: 1.0\n"
     ]
    }
   ],
   "source": [
    "import math as m\n",
    "H_e = m.log2(52)\n",
    "H_ta = (1/13)*m.log2(4) + (12/13)*m.log2(48) \n",
    "H_ta2 = (1/4)*m.log2(13) + (3/4)*m.log2(39)\n",
    "H_ta3 = (1/2)*m.log2(26) + (1/2)*m.log2(26)\n",
    "\n",
    "print(H_e)\n",
    "print(H_ta)\n",
    "print(f\"Information gain1: {H_e - H_ta}\")\n",
    "print(f\"Information gain2: {H_e - H_ta2}\")\n",
    "print(f\"Information gain3: {H_e - H_ta3}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4199730940219749\n",
      "0.4591479170272448\n"
     ]
    }
   ],
   "source": [
    "a = -2/3*m.log2(2/3) - 1/3*m.log2(1/3)\n",
    "b = -2/5*m.log2(2/5) - 3/5*m.log2(3/5)\n",
    "c = -4/5*m.log2(4/5) - 1/5*m.log2(1/5)\n",
    "\n",
    "\n",
    "X_0 = 6/10*a + 4/10*0\n",
    "X_1 = 1/2*c + 1/2*b\n",
    "X_2 = 1/2*b + 1/2*b\n",
    "\n",
    "print (b-X_0)\n",
    "\n",
    "X2_X10 = 4/6*a + 2/6*0\n",
    "X1_X10 = 3/6*a + 3/6*0\n",
    "\n",
    "print (a- X1_X10)\n",
    "\n",
    "X2_X11_X10 = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
