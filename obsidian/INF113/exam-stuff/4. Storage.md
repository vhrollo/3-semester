**A lot different has been used**
- punch-cards
- magnet tape
- magnet drum
- magnetic-core memory
- floppy
- optical
- flash

**Lowering CPU overhead with Interrupts**
*Instead of polling the device repeatedly, the OS can issue a request, put the calling process to sleep, and context switch to another task.*

![[Pasted image 20241114170007.png]]

**Problem**:
- When a process requests I/O (e.g., reading from disk), the CPU can either:
    - **Poll**: Continuously check if the I/O operation is complete (wasting CPU time).
    - **Use Interrupts**: Allow the CPU to work on other tasks and handle the I/O completion only when notified by an interrupt. 

**Solution with Interrupts**:
- **Interrupts** allow the CPU to handle other processes while waiting for I/O.
- When the I/O device completes the task, it sends an **interrupt** to the CPU, invoking an **Interrupt Service Routine (ISR)** to handle the completion.
- This improves CPU utilization by overlapping I/O and computation, as shown in the second timeline, where the CPU handles **Process 2** during the I/O wait time instead of idling.

![[Pasted image 20241114212836.png]]

**Another Problem with Programmed I/O (PIO)**:
- When transferring data between memory and a device (like a disk) using **Programmed I/O (PIO)**, the CPU must handle the data transfer word by word.
- This is inefficient, as it burdens the CPU with low-level data movement, wasting time that could be used for other processes.

**Solution with DMA**:

- **DMA (Direct Memory Access)** offloads data transfer work from the CPU to a **DMA controller**.
- The CPU sets up the DMA transfer by telling the DMA controller:
    - **What data** to transfer.
    - **Where** to transfer it.
    - **How much** data to move.
- Once configured, the DMA controller takes over the transfer, allowing the CPU to work on other tasks in parallel.

+ The **DMA controller** moves the data directly from memory to the device (or vice versa), as shown by "c" blocks in the DMA timeline.

- works in tandem with the storage device drivers

# HDD
*each sector holds 4 KB*

Spindle: the motor
Platter: the disks where the info is stored
Track: rings of data strips on the platter
sector: the strips splitted up into 4 KB sectors
- used to be 512 B
- its an standard

**HDD is slow**
- seek time
	- the time the head needs to move to right track
- rotational latency
	- how much it can wait until right sector is under the head
- transfer time

### HDD internal cache
- the disk has cache for incoming writes
	- write-back cache
	- write-through
- this is why cutting of the HDD can cause memory loss


# File Systems

- Linux
	- One singular root node
- Windows
	- Different "root" nodes per file

**Useful commands**
- fsync
- stat

**Example of stat used**
```bash
user@pc:~/Documents/uni/inf113/oblig3/oblig3-myfs$ stat main.py
  File: main.py
  Size: 3686      	Blocks: 8          IO Block: 4096   regular file
Device: 254,0	Inode: 5636140     Links: 1
Access: (0644/-rw-r--r--)  Uid: ( 1000/ vhrollo)   Gid: ( 1000/ vhrollo)
Access: 2024-11-12 17:26:45.374066658 +0100
Modify: 2024-11-10 15:56:34.339841696 +0100
Change: 2024-11-10 15:56:34.339841696 +0100
 Birth: 2024-11-05 18:16:18.315359272 +0100
```


**Memory flag**
$rwx\mid rwx \mid rwx$, where $user\mid group \mid others$
- That is why we have different "modes":
	- mode 755: $421\mid 4-1 \mid 4-1$
	- each number represent which flag is on

### Hardlinks and Softlinks

**Hardlink**: *hardlink ln /path/to/file /path/to/symlink*
- if file is removed it will not disappear
- will have equal privileges

**Softlink** *hardlink ln -s /path/to/file /path/to/symlink*
- don't affect original file
- don't have its own privileges

### Inodes blocks
- Will use a data block to have a certain amount of inodes in them
- These can point to other inodes (folders)
- They will include the names of the files
- Inodes can point to data structures
- When an inode doesn't fit its content in 12 blocks, it use direct pointers

**Example (Common ext4 Settings)**:
1. **Block Size**: 4096 bytes (default block size in ext4).
2. **Inode Size**: 256 bytes (default for ext4).
3. So here there would be 16 blocks
	1. Normally there is 12 direct pointers
	2. one singly
	3. one doubly
	4. one triple

### Bit Maps
+ Inode bitmap
+ Datablock bitmap
	- The first, and second is taken by the superblock

**File Type**
This will be saved in its top bits
- where we also have symlink type of inode


# Ext2 - File Systems
*SB | block group descriptor | block - bitmap | inode - bitmap | inodes | data*

**Example**: On how to format an FS
```bash
mkfs.ext2 -m0 -b 4k <name> 64 #64 blocks of 4kB, each block is seperated at x1000
```
- /dev/sda -> mount > /- you can make multiple partitions 
	+ this has to be done for it to be usable
- sudo chown, ch-own makes somebody else own the file

**Base super block fields** - *Information about the FS*
- Total number of inodes and total amount of blocks
- number of superuser blocks
- number of allocated blocks and inodes

**Block Group Descriptor** - x1000
- **Purpose**: Stores metadata about each block group; specifies locations of key structures within the group.

- **Key Fields (32 bytes per descriptor)**:
  - **0–3**: Block address of **block usage bitmap**.
  - **4–7**: Block address of **inode usage bitmap**.
  - **8–11**: Starting block address of the **inode table**.
  - **12–17**: Counts of unallocated blocks, unallocated inodes, and directories.
  - **18–31**: **Unused** (reserved for future use).

- **Location**: The Block Group Descriptor Table is immediately after the superblock.

- **Calculating Starting Point of Data Blocks**:
  - **Data Blocks** begin after the inode table in each block group.
  - **Calculation**:
    - **Starting Block of Data Blocks** = **Starting Block of Block Group** + **Offset**.
    - **Offset**:
      - **+1 block** for the **block bitmap**.
      - **+1 block** for the **inode bitmap**.
      - **+ (Inode Table Size in blocks)**.
      - **+ (Backup superblock and descriptor table size)** if present.
    - **Inode Table Size (in blocks)**:
$$\text{Inode Table Size} = \left\lceil \dfrac{\text{Inodes per Group} \times \text{Inode Size}}{\text{Block Size}} \right\rceil$$
- **Note**: The starting address of data blocks isn't stored in the descriptor—it’s calculated based on the sizes of preceding structures within the block group.


**Inode 1**
this will refer to to which block have directory of top-level


**Example**: when saving to a file
- Update bitmap
- Write to inode
- Write to Datablock

### Lookup
# Address Calculations

- **Address of Block \( N \):**
$$
N \times 0x1000
$$
- **Address of Inode \( i \):**
$$
0x4000 + (i - 1) \times 0x100
$$

### Block Groups ( Fast File System )
*when data ends up really far away from an inode, one typically makes block groups*

- Related stuff is on the same Block Group
- Folders are evenly distributed along BGs
- Data and inode is in same BG
- big file is then splitted over multiple BGs



### UTF-8
*lecture side quest*

| Bytes | Bit Pattern                                 | Bits | Code Point Range    |
| ----- | ------------------------------------------- | ---- | ------------------- |
| 1     | `0xxxxxxx`                                  | `7`  | U+0000 to U+007F    |
| 2     | `110xxxxx` `10xxxxxx`                       | `11` | U+0080 to U+07FF    |
| 3     | `1110xxxx` `10xxxxxx` `10xxxxxx`            | `16` | U+0800 to U+FFFF    |
| 4     | `11110xxx` `10xxxxxx` `10xxxxxx` `10xxxxxx` | `21` | U+10000 to U+10FFFF |

## Crash Consistency
*When a file requires multiple writes it could technically fail mid write*

**A new file needs to write in**
- two bitmaps, inode, data
- also folder-data
- folder needs needs more space
	- bitmaps, inode, data

### File system checker (`fsck`)
**It cheks**:
- superblock
- free blocks: bitmaps <-> inodes
- inode state
- inode links
- duplicate pointers
- bad blocks
- directory checks
#### Examples of what happen when it fails:
*bitmap | inode | datablock*

**Just one write goes through**:
- Reserved space, but inode doesn't point to it, *space leak*
	-  less dangerous, 1 datablock is momentarily useless till `fsck` fixes it
- The datablock is not updating the data datablock or the bitmap
	- DB just gets reused, `fsck` doesn't care
- only inode update goes through, Inode points to whatever is in in the expanded block
	- dangerous
	- `fsck` fixes the bitmap, but not the content


**Only one write goes through**
- If you miss the pointer, it will be a *space leak*
	- `fsck` fixes
- The datablock is not being written, then it will just point to garbage data
	- `fsck` doesn't care, and can't fix
- Inode points to a datablock, and the bitmap is not updated
	- this means that the block can get overwritten, and then another process can read its data
	- `fsck` fixes the bitmap, so no other can use the same block

# Journaling