
# Journaling
*used in ext3 and ext4*

Used to avoid situations where not everything gets written, so we use a journal to ensure everything gets. Basically a way to write to a continues segment, before writing it out to the storage. *Kind of a checkpoint*

After it is written to the disk, the journal entry gets cleared, and if energy goes while it writes before it clears, then it just begins writing to disk again.

`super | Journal | Group 0 | Group 1 | ... | Group N`

In the journal you write down what is going to change:
`TxB | I[v2] | B[v2] | Db | TxE`

### Consistency
- only ready when TxE is ready
- in ext4 it calculates a checksum, which will be placed in TxB 
	- TxB is checksum is self-contained as it does not depend on the following blocks 
		- The only thing that is used is the meta data about the transaction:
			- transaction ID, sequence number
			- space reserved for the checksum of the TxB block
	- Every block has a checksum which ensures that the data has not been corrupted during write
	- Unlike the TxB checksum, which only covers the TxB block, the checksum in the TxE block is calculated over **all blocks in the transaction**, including:
		- The TxB block.
		- Descriptor blocks (if present).
		- Data blocks.
		- Metadata blocks.
- This way, it can validate if the system is valid

- We are only allowed to do this because we have faster CPUs

### Consistency via transactions
*Circular log*
2 pointers is used, one to start, and one to the end, only update pointers when a checkpoint is done. This way we get a record of all the transactions needed to be done


# RAID (redundant array of independent disks)
*When FS need to be big, fast, and reliable*

The main idea behind it, is that one FS have multiple physical disks, which then can use multiple I/O at the same time, increase the memory drastically, or add redundancy. The RAID should look like one disk for the OS.

#### Level 0 - striping
- No redundance, each disk fault will lead to data loss.
- Shows what is maximal possible in capacity and performance
- Data blocks are spread evenly out, chuck size is difficult to choose
- Chunk size is how many blocks you will save in one disk before you go to the next disk

![[Pasted image 20241119162600.png]]
- *here 1 chunk is used in the left, and 2 in the right one*
- size: N disk with B blocks = $N\cdot B$
- redundance: None
- Performance:
	- Sequential: $N\cdot S$
	- Random: $N \cdot R$

### Level 1: Mirroring
- Every block is copied to another disk
- Size: $N/2 \cdot B$
- Redundance: 1 disk fault ( up to N/2 if you are lucky)
- Performance:
	- Sequential write: $N/2 \cdot S$, seq. read: $N/2 \cdot S$
		- This is mostly to ensure that the data is read in a logical sequence
	- Random read: $N\cdot R$, random write $N\cdot R/2$


### Level 4: Parity
- We use a whole disk as a "parity"
- Each stripe gets a parity block
- Here we can use XOR
- We then know if a bit has been flipped
- This way we can handle one whole disk fault
- Size: $(N-1) \cdot B$ 
- Redundance: 1 disk fault
- Performance: 
	- Sequential reads $(N-1) \cdot S$,  Sequential write: $(N - 1) \cdot S$ ( only one stripe at a time).
	- Random Read $(N-1) \cdot R$
	- Random write can be done in parallel as long as the same stripe is not being written to at the same time.
		- This is limited to the parity disk, as it can only write one parity block at a time
		- It is $R/2$, since it has to write the new parity for every stripe update.

### Level 5: Distributed Parity
- Splits the workload for the parity across the disks
- Random write $N/4 \cdot R$ 

# Log-structured FS
*If we have extra space lying around, why not use it as RAID does.*
*It will use copy-on-write*

It will gather data which will be written to memory, and do it in one long sequential write, where it will always pick a new part of the memory.
- Multiple writes gets gathered together in a write buffer


