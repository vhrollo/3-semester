*what is meant by efficient algorithms*

# Computational Tractability
- Identifying the broader themes of the running bound of a function
- Checking it, we can compare it to brute-force

**Generic Definition of Efficiency**
- *An algorithm is efficient if it achieves quantitatively better worst-case performance, at an analytical level, then brute-force search*

- A more appropriate way to classify algorithms is within $cN^d$, that it is a polynomial-time algorithm

**Polynomial Time as Definition of Efficiency**
- *An algorithm is efficient if it has polynomial running time*

**Resources**
- Time
- Space
- Others
	- Lock
	- File Handles
	- Physical devices

- Time $\geq$ space

**Constants**
- Different programming languages use fewer instructions
- We only care about the general growth, so we ignore constants

---

# Notions 
*How to explain the growth of functions*

**Upper Bounds** - *O*
 - $T(n) = O(f (n))$   representing the upper asymptotic bound
 - to get the definition into $cN^d$ we turn $T(n) = pn^2 + qn + r \leq pn^2 +qn^2 +rn^2 = (q + p +r)n^2$ 
	- $T(n) \leq cn^2$ and $c = (q + p +r)$
- If we calculate that the upper bound of a function is $O(n^2)$, it is also right that the upper bound is $O(n^3)$, only that it isn't as tight as it could be
- $f(n) \in O(n^2)$
- f is O(g), then $f\leq g$ 
- Example:
	- $16n^2 + 35n + 8 \leq 16n^2 + 35n^2 + 8n^2 = 59n^2$

**Lower Bounds** - $\Omega$
- $T(n)$ is $\Omega(f(n))$,  where an  constant $c * f(n) \leq T(n)$
- $T(n) = pn^2 + qn + r \geq pn^2$ 
- a weak lower bound could be $\Omega(n)$

**Tight Bounds** - $\Theta$ 
- If $O(f(n))=\Omega(f(n))$, then the asymptotically tight bound would be:
	- $T(n) = pn^2 + qn + r$ is both $O(n^2)$ and $\Omega(n^2).$ then $\Theta(n^2)$
- Transitivity holds:
	- g is upperbounded by h, and h is upperbounded by f, g would then be upperbounded by f

### Common Running Times

**Polynomials**
- $n^2$
- $n^{\frac{1}{2}}$ 
- We use the highest polynomial
- $O(n\log(n))$ also is seen as polynomial, since it grows slower than $O(n^2)$

**Logarithms**
$$log_a n = \frac{log_b n}{log_b a}$$
- The base of logarithm is not needed, as $\frac{1}{\log_b a}$ is just a constant.
- It depends on bases a and b, which are constants. Therefore each logarithmic growth is just a multiplied by a constant from each other

**Exponentials**
- An **exponential function** is of the form $f(n) = r^n$, where r is a constant base and r>1.
- $n^d = O(r^n), r>1,  d>0$ 
- This means that every exponential function grows faster than any polynomial function as n becomes large. In other words, no matter how large d is, eventually $r^n$ will outgrow $n^d$ because the exponential function grows much faster.
-  $r^n≤c⋅s^n$
	- there is no c where $s^n$ is true for all sufficiently large n
- The statement rules out the possibility that $s^n$ can bound $r^n$ if $r>s$. This highlights the hierarchy of exponential growth:
    - If $r>s$, $r^n$ "outgrows" $s^n$, and no constant c can compensate.
    - This is a result of how exponentially growing functions dominate each other based on their bases.

---
### A Survey of Common Running Times
This section surveys commonly occurring running times of algorithms and relates them to practical applications:

1. **Constant Time ($O(1)$):**
   - Fixed execution time, regardless of input size.
   - Examples: Array indexing, simple arithmetic operations.

2. **Logarithmic Time ($O(\log n)$):**
   - Time grows proportionally to the logarithm of input size.
   - Examples: Binary search.

3. **Linear Time ($O(n)$):**
   - Time grows directly proportional to the input size.
   - Examples: Scanning an array, finding the maximum element.

4. **Linearithmic Time ($O(n \log n)$):**
   - Combines linear and logarithmic growth.
   - Examples: Efficient sorting algorithms like merge sort and heap sort.

5. **Quadratic Time ($O(n^2)$):**
   - Time grows as the square of the input size.
   - Examples: Simple nested loops, brute-force pairwise comparisons.

6. **Cubic Time ($O(n^3)$):**
   - Time grows as the cube of the input size.
   - Examples: Naive matrix multiplication.

7. **Exponential and Factorial Time ($O(2^n)$) and ($O(n!)$):**
   - Extremely rapid growth, typically for brute-force solutions to hard combinatorial problems.
   - Examples: Traveling Salesman Problem, generating permutations.


---

### A More Complex Data Structure: Priority Queues
This section introduces **priority queues**, a versatile data structure for managing elements with associated priorities:

1. **Definition:**
	- Allows for efficient insertion and retrieval of elements based on priority.
	- Two main operations:
		- `Insert(x)`: Add an element.
		- `Extract-Min` or `Extract-Max`: Remove and return the element with the highest or lowest priority.

2. **Implementation:**
	- https://www.cs.usfca.edu/~galles/visualization/Heap.html
	- **Heap-based implementation**:
	     - Binary heaps are common due to logarithmic time for `Insert` and `Extract`.
	     - Maintains a partially ordered tree structure:
		     - `Insert`: $O(\log n)$
		     - `Extract-Min`/`Extract-Max`: $O(\log n)$
	     - Example: A Min-Heap keeps the smallest element at the root.

3. **Applications:**
	- **Dijkstra's Shortest Path Algorithm**: Manage exploration of vertices efficiently.
	- **Event Scheduling**: Process events in priority order during simulations.
	- **Greedy Algorithms**: Applications like Huffman coding and Prim's MST.


**Short on Heaps**

**Min-Heap:**
- The value of the parent node is less than or equal to the values of its children.
- The root node contains the smallest element.
**Max-Heap:**
- The value of the parent node is greater than or equal to the values of its children.
- The root node contains the largest element.

**Insertion ($O(\log(n))$):**
- Add the new element at the end of the tree (to maintain completeness).
	- Bubble Up (or Heapify-Up) the new element:
		- Compare it with its parent.
		- Swap if the heap property is violated.
		- Repeat until the heap property is restored or the element reaches the root.

**Extract-Min/Extract-Max ($O(\log(n))$):**
- Remove the root (smallest in Min-Heap, largest in Max-Heap).
- Replace the root with the **last element** in the tree.
- **Bubble Down** (or **Heapify-Down**) the new root:
    - Compare it with its children.
    - Swap with the smaller child (Min-Heap) or larger child (Max-Heap) if the heap property is violated.
    - Repeat until the heap property is restored or the element becomes a leaf.

**minheap impl for python**
```python
class MinHeap:
    def __init__(self):
        self.heap = []

    def parent(self, i):
        return (i - 1) // 2

    def left(self, i):
        return 2 * i + 1

    def right(self, i):
        return 2 * i + 2

    def insert(self, value):
        self.heap.append(value)
        self._heapify_up(len(self.heap) - 1)
    
    def extract_min(self):
        if len(self.heap) == 0:
            return None
        if len(self.heap) == 1:
            return self.heap.pop()
        
        root = self.heap[0]
        self.heap[0] = self.heap.pop()
        self._heapify_down(0)
        return root

    def _heapify_down(self, i):
        left = self.left(i)
        right = self.right(i)
        smallest = i

        if left < len(self.heap) and self.heap[left] < self.heap[i]:
            smallest = left
        if right < len(self.heap) and self.heap[right] < self.heap[smallest]:
            smallest = right
        if smallest != i:
            self.heap[i], self.heap[smallest] = self.heap[smallest], self.heap[i]
            self._heapify_down(smallest)

    def _heapify_up(self, i):
        while i != 0 and self.heap[self.parent(i)] > self.heap[i]:
            self.heap[self.parent(i)], self.heap[i] = self.heap[i], self.heap[self.parent(i)]
            i = self.parent(i)
```