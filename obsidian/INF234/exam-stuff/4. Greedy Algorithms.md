4.1 Interval Scheduling: The Greedy Algorithm Stays Ahead  
4.2 Scheduling to Minimize Lateness: An Exchange Argument  
4.3 [not covered]  
4.4 Shortest Paths in a Graph  
4.5 The Minimum Spanning Tree Problem  
4.6 Implementing Kruskal's Algorithm: The Union-Find Data Structure  
4.7 Clustering  
4.8 Huffman Codes and Data Compression  
4.9 [not covered]


# **Normal Greedy Algorithms**
*algorithm which optimizes on the spot, no takesies-backsies*
## Interval Scheduling
*basically max the number of intervals you can schedule*

**Input**
- $I = \{I_1, I_2,..., I_n\}$

**Output**
- Maximum set of pairwise non overlapping intervals

**Psudocode**
```
Initially let R be the set of all requests, and let A be an empty set

While R is not empty
	Choose a request i in R that has the smallest finishing time
	Add request to i to A
	Delete all requests from R that are not compatible with request i

Return set A
```

**Analyzing the Algorithm**
- Given that A is our solution, $|A|=|O|$, or $A \in O$ where O can be a set of solutions.
- **Inductive proof** that our algorithm will stay ahead
	- **Base case:** Our greedy algorithm guarantees us that $f(i_1)\leq f(j_1)$, where *i* is the first segment in our greedy algorithm, and *j* is another segment in the optimal solution set.
	- **Inductive step:** $r \geq 1$, we will assume that it is true for $r-1$
		- $f(i_{r-1}) \leq f(j_{r-1})$ then $f(i_{r}) \leq s(j_{r})$, we know that $j_r$ is one of the potential candidates for $i_{i-1}$, and it did not choose it, therefore $f(i_{i}) \leq f(j_{r})$ must be true
- The greedy algorithm returns an optimal set A
	- The statement asserts that the greedy algorithm produces an optimal solution **A**, comparable to an optimal solution set **O**, because of its "staying ahead" property. This property ensures that at every step of the algorithm, the greedy choice guarantees at least as good a solution as any optimal solution would provide up to that point.

![[Pasted image 20241203200649.png]]

## Interval Partitioning
**Input**
- Set of intervals: $\{I_1, I_2, \dots, I_n\}$
**Output**
- Partition a set of intervals $\{I_1, I_2, \dots, I_k\}$ into the fewest possible subsets of pairwise non-overlapping intervals.

**Key Definitions:**
- **Depth ($\text{depth}(\mathcal{I})$):**  
  The maximum number of intervals that overlap at any point in time $t$:  
  $$\text{depth}(\mathcal{I}) = \max_t \left| \{ I \in \mathcal{I} : s(I) \leq t \leq f(I) \} \right|$$
  Here, $s(I)$ and $f(I)$ are the start and finish times of interval $I$.

**Algorithm:**
- **Input:** Intervals $\{I_1, I_2, \dots, I_n\}$, assume start and finish times are unique (WLOG).  
- **Output:** An assignment of intervals to resources (colors) such that no overlapping intervals share the same resource.
	1. Sort intervals by their start times.
	2. Use a **stack of colors** (this equals depth) to assign resources:
	   - When an interval starts, pop a color from the stack and assign it to the interval.
	   - When an interval ends, push its color back to the stack for reuse.

**Shown visually**
![[Pasted image 20241203200314.png]]

**Corollary:**
- For interval graphs, the size of the **maximum clique** is equal to its **chromatic number** (the minimum number of colors needed to color the graph).

**Lemma:**  
The algorithm colors every interval using $\text{depth}(\mathcal{I})$ colors.  
- **Proof:**  
  At any time $t$, the number of available colors is:
  $$
  \text{depth}(\mathcal{I}) - \left| \{ I \in \mathcal{I} : s(I) \leq t \leq f(I) \} \right|
  $$
  Colors ending before $t$ are popped, and colors are reused after intervals end.  

**Claim:**  
There is an available color at time $t = s(I)$ *before* it is assigned to interval $I$.  
- **Proof of Claim:**  
  Suppose $k$ intervals started but did not finish before $s(I)$.  
  At $t' = s(I)$, there are $k + 1$ overlapping intervals.  
  Since $\text{depth}(\mathcal{I}) = d$, we have:
  $$
  k + 1 \leq d \implies d - k \geq 1
  $$
  Thus, at least one color is available.



## Lateness Scheduling
**Input:**  
- Requests $\mathcal{I} = \{I_1, I_2, \dots, I_n\}$
**Output:**
- Assign start times to each request such that no two requests overlap and the **maximum lateness** is minimized.

**Definitions:**
- **Request $I_i$:**
	- $t_i$: Processing time of request $I_i$.
	- $d_i$: Deadline of request $I_i$.
	- $f(i)$: Finish time of request $I_i$, where $f(i) = s(i) + t_i$.
- **Lateness $L_i$:**
$$L_i = \max(0, f(i) - d_i)$$
- **Maximum Lateness:**
$$L_{\max} = \max_i L_i$$


### Earliest Deadline First (EDF) Algorithm
1. Sort the requests by their deadlines:
$$d_1 \leq d_2 \leq \dots \leq d_n$$
2. Initialize $s_1 = 0$.
3. For each request $i = 1, \dots, n$:
   - Assign $I_i$ to the interval $[s_i, s_i + t_i)$.
   - Update the current time: $s_{i+1} = s_i + t_i$.

#### Theorem  
The **Earliest Deadline First (EDF)** algorithm minimizes the maximum lateness $L_{\max}$.

#### **Proof**
**Definition of an Inversion:**
- In a schedule $S$, an **inversion** is a pair of requests $(i, j)$ such that:
  - $i < j$ (i.e., $d_i \leq d_j$),
  - But in the schedule $j$ comes before $i$.

 **Observation:**
- Any **idle-free** schedule (where there are no gaps between tasks) has a unique ordering of requests if there are no inversions.

 **Claim:**  
If an idle-free schedule has an inversion, it also has a **back-to-back inversion**:
- A **back-to-back inversion** is the closest possible inversion $(i, j)$ where no other requests occur between $i$ and $j$ in the schedule.
- Adding a task between $i$ and $j$ would violate the definition of a back-to-back inversion and create a contradiction. *(did not understand this >.>)*



#### Lemma: Exchange Argument
- **Statement:**  
  Exchanging two adjacent inverted requests $(j, i)$ in the schedule reduces the number of inversions without increasing the maximum lateness $L_{\max}$.
  
- **Proof:**
  1. When two adjacent requests $(j, i)$ are swapped such that $d_j \leq d_i$, the lateness of $i$ and $j$ cannot increase because:
     - Request $j$ now finishes earlier or at the same time, reducing or maintaining its lateness.
     - Request $i$ starts at the same time or later, preserving its lateness.
  2. This exchange decreases the number of inversions while ensuring $L_{\max}$ does not increase.
	  - This can decrease the lateness as well
**vis**
![[Pasted image 20241203203159.png]]
- By repeatedly applying the **exchange argument**, any idle-free schedule with inversions can be transformed into a schedule without inversions, aligning with the order $d_1 \leq d_2 \leq \dots \leq d_n$. This corresponds to the schedule produced by the EDF algorithm, proving that EDF minimizes $L_{\max}$.


---
# Weighted Greedy Algorithms
*Accounting the cost, weight, or some other metric up to segment $\mathcal{I}$*

## Dijkstra's Algorithm
- **Limitation:** This algorithm does not work with negative edge weights.
- **Graph Representation:**  
	- $G = (V, E)$, with edge weights $l: E \to \mathbb{N}$  
	- $dist_G(u, v)$: Shortest path distance in the graph.  
	- $l(P)$: Sum of edge weights in path $P$.

**Single-Source Shortest Path Problem**
- **Input:**  
  $G = (V, E)$, $l: E \to \mathbb{N}$, and a source vertex $s \in V$.
- **Output:**  
  $dist_G(s, t)$ for all $t \in V$ (shortest distances from $s$ to all other vertices).  

**Algorithm Idea**
- Maintain a set $S \subseteq V$, where the correct shortest path distances $d(v)$ are known for all $v \in S$.
- Use:
	- $d(v)$: Final shortest distance from $s$ to $v$.  
		- the finalized shortest distance
	- $d'(v)$: Tentative shortest distance from $s$ to $v$ (continuously updated).  
		- discovered as the current shortest path to $v$ (not yet searched)

**Algorithm Steps**
1. Initialize:
   - $d'(v) = \infty \; \forall v \in V$
   - $d'(s) = 0$
   - $S = \emptyset$
2. While $S \neq V$:  
   - Pick $v \in V - S$ with the smallest $d'(v)$ (using a priority queue).  
   - Add $v$ to $S$.  
   - $d(v) = d'(v)$ (finalize the distance for $v$).  
   - For each $u \in N(v) \setminus S$ (neighbors of $v$ not in $S$):  
     - If $d(v) + l(vu) < d'(u)$:  
	     - Update $d'(u) = d(v) + l(vu)$  
	     - Set $pre(u) = v$ (store the predecessor of $u$).  
3. Return $d$ (final distances) and $pre$ (predecessor pointers).

**Complexity**
- $O(n)$: Iterating through vertices.
- $O(\log n)$: Extracting the smallest element from the priority queue (per vertex).  
- $O(m)$: Checking neighbors (edges).  
- $O(\log n)$: Updating priority queue (per edge).  

**Overall Time Complexity:** $O((n + m) \log n)$.

#### Proof: Correctness of Dijkstra's Algorithm

**Theorem:**  
Dijkstra's algorithm correctly computes the shortest distances $d(v)$ from $s$ to all vertices $v \in V$.

**Proof Structure:**
1. **Base Case:**  
   - When $|S| = 1$, $S = \{s\}$, and $d(s) = 0$.  
   - This is trivially true since $s$ is the source vertex.

2. **Induction Hypothesis:**  
   - Assume the algorithm has correctly computed $d(v) = dist_G(s, v)$ for all $v \in S$, where $|S| = k - 1$.

3. **Induction Step:**  
   - In step $k$, let $v$ be the vertex added to $S$ with the smallest tentative distance $d'(v)$.  
   - Consider the path $P_{s \cdot v}$:  
     - Claim: $P_{s \cdot v}$ is the shortest $s$-to-$v$ path.  
   - **Proof of Claim:**
     - Suppose there exists a shorter $s$-to-$v$ path $P_{s \cdot u \cdot v}$.  
     - Since $u \in S$, we know $d'(v) \leq d'(u)$.  
     - This implies that the path through $u$ cannot be shorter than $P_{s \cdot v}$, contradicting the assumption.  
   - Hence, $P_{s \cdot v}$ is the shortest path.  
   - The claim holds, and $d(v) = dist_G(s, v)$.  

By induction, the algorithm is correct for all vertices $v \in V$.



### **Minimum Spanning Trees (MST)**

#### Definition
Let $G = (V, E)$ be a weighted graph with edge weights $l : E \to \mathbb{N}$, and let $T = (V, E')$ be a subgraph of $G$. The following are equivalent (TFAE):
1. $T$ is a **spanning tree** of $G$.
2. $T$ is **connected** and **acyclic**.
3. $T$ is **acyclic** and $|E'| = |V| - 1$.
4. $T$ is **minimally connected** (removing any edge from $T$ disconnects it).
5. $T$ is **maximally acyclic** (adding any edge to $T$ creates a cycle).

#### Problem Statement
- **Input:** $G = (V, E)$, $l : E \to \mathbb{N}$.  
- **Output:** A spanning tree $T = (V, E')$ such that the sum of edge weights $l(E')$ is minimized.

### Prim's Algorithm
**Idea:**  
- Grow the MST incrementally by always adding the smallest edge that connects the tree to a new vertex.

**Algorithm Steps:**
1. Initialize:
   - Let $S = \emptyset$ (MST starts empty).
   - $c(v) = \infty$ for all $v \in V$ (tentative costs).
   - Pick an arbitrary starting vertex $s \in V$: $c(s) = 0$.
2. While $S \neq V$:
   - Pick $v \in V \setminus S$ with the smallest $c(v)$.
   - Add $v$ to $S$.
   - For each $u \in N(v) \setminus S$:
     - If $l(vu) < c(u)$:
	     - Update $c(u) = l(vu)$.
	     - Set $pre(u) = v$ (store predecessor to reconstruct tree).
1. Return the edges in the MST.

**Prim's Algorithm Invariant** (invariant means that it ensures correctness throughout the algorithm)
- At every step, $S$ is a subset of the MST, and the algorithm always adds the **cheapest edge** that connects $S$ to a vertex outside $S$.


### **Kruskal's Algorithm**
**Idea:**  
Grow the MST by repeatedly adding the smallest edge that does not form a cycle.

**Algorithm Steps:**
1. Initialize:
   - Start with an empty tree $T = (V, \emptyset)$.
   - Sort all edges $E$ by their weights $l(e)$.
2. For each edge $e \in E$ (in ascending order of weight):
   - If adding $e$ to $T$ does not create a cycle:
     - Add $e$ to $T$.
3. Return $T$.


#### **Key Differences Between Prim's and Kruskal's**
| **Aspect**         | **Prim's Algorithm**                   | **Kruskal's Algorithm**                    |
| ------------------ | -------------------------------------- | ------------------------------------------ |
| **Approach**       | Greedy vertex-based growth.            | Greedy edge-based growth.                  |
| **Data Structure** | Priority queue for vertices.           | Union-Find for cycle detection.            |
| **Edge Selection** | Adds the cheapest edge connecting $S$. | Adds the smallest edge that avoids cycles. |
| **Best For**       | Dense graphs (fewer edges to process). | Sparse graphs (fewer edges initially).     |

#### **Complexity**
1. **Prim's Algorithm:**
   - **Using Priority Queue:** $O((n + m) \log n)$, where $n = |V|$ and $m = |E|$.
2. **Kruskal's Algorithm:**
   - **Using Union-Find:** $O(m \log m)$ (sorting edges dominates).

#### Proof of Correctness 

**Lemma (Cut Property):**
- Let $G = (V, E)$, $l : E \to \mathbb{N}$ be a weighted graph with unique edge weights.  
- Let $S \subset V$, and let $e = vw$ be the unique smallest edge such that $v \in S$ and $w \notin S$. Then every MST contains $e$.

**Proof (Exchange Argument):**
1. Let $T$ be a spanning tree that does not contain $e$.  
   - We will show that $T$ is not minimal.
2. On the path from $v$ to $w$ in $T$, let $e'$ be the unique edge leaving $S$.
3. Replace $e'$ with $e$:
   - Let $T' = T - e' + e$.
4. **Key Observations:**
   1. $T'$ has strictly smaller weight than $T$ because $l(e') \geq l(e)$.
   2. $T'$ is still a spanning tree because:
      - All vertices are connected.
      - The number of edges $|E'| = |V| - 1$, so $T'$ is acyclic.
5. Therefore, $T$ is not a minimum spanning tree.

**Shown visually**
![[Pasted image 20241203215805.png]]

---

# Clustering
*Given a collection of data points, we want to put them in coherent groups. An application of MST algorithms*

**Clustering of Maximum Spacing**
- K-clustering
	- We want to partition the set of points into k clusters such that for $p,q \in C_i, C_j, i\not = j$  that minimum distance is maximized
- Spacing of a cluster
	- Defined as the **minimum distance between any pair of points that are in different clusters**.
		- Given clusterings  $\mathcal{C} = C_1, ... , C_k$  
		- The spacing of $\mathcal{C}$ is min $d(p,q)$ with $p \in C_i, q \in C_j, j \not = i$ 
	- Our goal is to **maximize** this spacing.

- Suppose we have some points in the plane with  $dist(p,q) = \sqrt{(p_x - q_x)^2 + (p_y - q_y)^2}$  (euclidean)

- Kruskal's algorithm, but stopped early, so there is k many clusters left
	- We then want to stop at the $k - 1$ edge left
	- Build whole MST, then remove the $k-1$ latest edges

- The sum of minimum spacing is done differently and is NP-complete

**Observation**
- We iteratively add cycles in Kruskal's 
- When we add $p, q$ the spacing is $\geq d(p,q)$
- Proof:
	- Because we add $p, q$ all edges with smaller distances has been considered

**Lemma**
Let $\mathcal{C}^* = C_1^*, ..., C_k^*$ be the clustering found by Kruskal's after adding n - k edges
- The $C^*$ is a k-clustering of max spacing
- Proof:
	- Let $\mathcal{C} = C_1, ..., C_k$ be any other k - clustering 
	- show spacing of $\mathcal{C}^* \geq$ spacing of $\mathcal{C}$
	- Let $P_j \cdot P \in C_r^*$, but $P_i \in C_S, P_j \in C_t$
	- The spacing of $\mathcal{C}$ is the length of the (k-1) longest edge in the MST
	- $P_i \cdot P_j \in \mathcal{C}_r^*$
	- Some edge $(q,p)$ on the $P_i \cdot P_j$ path end up in different clusters in $\mathcal{C}$
		- Since $P_i$​ and $P_j$​ are in different clusters in $C$, there must be at least one edge $(p,q)$ along the path where:
			- Point p is in one cluster in C.
		    - Point q is in a different cluster in C.
		- This edge $(p,q)$ is called a cut edge with respect to C.
	-  The spacing of $\mathcal{C} \leq d(p,q)$
		- there could be another point which is smaller
	- In $C∗$, all edges within clusters are shorter than or equal to the smallest of the $k−1$ longest edges removed from the MST.
		- Since $(p,q)$ is an edge within a cluster in $C^∗$, its length $d(p,q)$ is less than or equal to the minimal inter-cluster distance in $C^∗$.
			- Therefore, spacing of $C^∗≥d(p,q)$.
	

![[Pasted image 20241204125422.png]]


---

# Data Compression 
*If the text is n long, the encoding $\cdot$ n*

- We want frequent symbols to have a shorter encoding

**Def. (prefix code)**
- A prefix code is an encoding which no symbols is a prefix of another's
## Hoffman Encoding
*A binary tree in which every internal node has two children. The leaves are labeled with symbols*
- Encoding of a symbol gives the path in a tree from root to the label

**Huffman codes**
- Encode (compress) messages using variable-length encoding

- Suppose that the alphabet is $\Sigma_i$, and $f_{\sigma}$ is the frequency in the message.
	- Note: If our message  M has length n, then $n \cdot f_\sigma$ is the number where $\sigma$ appears in message M.
- Let $\gamma$ : $\Sigma \rightarrow B^*$
- $n \cdot f_\sigma \cdot |\gamma (\sigma)| \rightarrow$ how much $\sigma$ contributes to the total length of the message 

- The **total** length will then be:
	- $\sum_{\sigma \in \Sigma} n \cdot f_\sigma |\gamma (\sigma) | = n \cdot \sum_{\sigma \in \Sigma} f_\sigma |\gamma (\sigma)|$
	- Average bit length: 
		- $\sum_{\sigma \in \Sigma} f_\sigma |\gamma (\sigma)|$

#### Algorithm for computing Hoffman Encoding

**Encoding**:
- input Alphabet Sigma, $f_\sigma$ frequencies
- output: encoding of Sigma that minimizes average bit length

Expansion for finding a prefix tree minimizing ABL
- $\sum_{\sigma \in \Sigma} \cdot f_\sigma \cdot depth(\sigma)$

**Definition**
- A full binary tree is a tree where every internal code has exactly two children (0 if they are leaf nodes)

**Claim:**
- An optimal prefix code can be represented by a **full binary tree**.
- **Proof:**
	- **Assumption**: Suppose there's an optimal tree T with an internal node having only one child.
	- **Contradiction**: We can remove this redundant node without affecting the prefix property, reducing the average depth and thus the ABL.

**Lemma:**
- In an optimal tree T, if $f_x < f_y$​ then $\text{depth}_T(x) \geq \text{depth}_T(y)$.
	- **Intuition**: Less frequent symbols should have longer codewords.

- **Proof**:
	- Assume, for contradiction, that $f_x < f_y$ but $\text{depth}_T(x) < \text{depth}_T(y)$. Swap the depths of $x$ and $y$. The change in average bit length (ABL) is:
$$\Delta \text{ABL} = (f_x - f_y) \left( \text{depth}_T(y) - \text{depth}_T(x) \right)$$
	- Since $f_x - f_y < 0$ and $\text{depth}_T(y) - \text{depth}_T(x) > 0$, it follows that $\Delta \text{ABL} < 0$, meaning the ABL decreases. This contradicts the assumption that $T$ is optimal. Therefore, $\text{depth}_T(x) \geq \text{depth}_T(y)$ in an optimal tree.

**Algorithm**
```psudo
Initialize priority queue Q with (f_v, v) for each symbol v in Σ
while |Q| ≥ 2:
    (f_1, v_1) = pop(Q)  // Node with lowest frequency
    (f_2, v_2) = pop(Q)  // Node with next lowest frequency
    Create a new node v with frequency f_v = f_1 + f_2
    Set v.left = v_1, v.right = v_2
    Insert (f_v, v) back into Q
Return the remaining node in Q as the root of the Huffman tree
```

**Example**
*where the smallest go left on 0 encoding*
$Q = \{(5, A), (9,B), (12,C)\}$
pop A, B
![[Pasted image 20241204143019.png]]

$Q = \{(14, AB), (12,C)\}$
pop C, and AB
![[Pasted image 20241204143143.png]]
