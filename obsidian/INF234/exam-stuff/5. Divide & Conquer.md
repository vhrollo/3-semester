*Simply divide tasks into smaller tasks, and then solve each smaller task, and then combine them for the overall solution*
# Merge and Recurrence Relations

**Statically Divide & Conquer** - Merge Sort
- The problem is independent from the input data
- The problem is split into **equal-sized** subproblems, regardless of the content of the data
- The division strategy is **fixed** and **systematic**

**Dynamically Divide & Conquer** - Quick Sort
 - Dependent on the input data, the way it is divided changes dynamically
 - The problem is split into subproblems of **variable sizes**
 - adaptive

**Notion** - $T(n)=2\cdot T(n/2)+cn$
 - $2$ is the number of recurrences
 - $T(n/2)$ is how big each recurrence is
 - $c\cdot n$ is how much work happens at each recurrence

**Notion visually:**
![[Pasted image 20241204180729.png]]

**Unrolling the Merge Sort Recurrence**
- $T(n)=2\cdot T(n/2)+cn$
- Each level contributes $2^j \left( c \cdot \frac{n}{2^j} \right) = c n$ time.
- Summing over the $\log_2 n$ levels, the total time is bounded by $O(n \cdot \log n)$.

#### **Partial Substitution On Merge Sort**
*Partial substitution involves **assuming a solution form** for the recurrence and then **verifying** that this assumption holds by substituting it back into the recurrence relation.*

- Assume $T(n) \leq k \cdot n \cdot \log_b n$.
- Then:
	- $T(n) \leq 2k \cdot \frac{n}{2} \cdot \log_b \left( \frac{n}{2} \right) + c n$
- Simplify:
	- $T(n) \leq k n \cdot \log_b \left( \frac{n}{2} \right) + c n$
- Using $b = 2$:
	- $T(n) \leq k n (\log_2 n - 1) + c n$
	- $T(n) \leq k n \log_2 n - k n + c n$
	+ $T(n)+kn-cn \leq k n \log_2 n$
- To satisfy $T(n) \leq k n \log_b n$, we need: 
	- $k n - c n \leq 0 \implies k \leq c$

#### Merge Sort Recurrence Relation
Suppose we spend $T(n)$ time on sorting a list of size $n$ with merge sort:
- $T(n) = 2 T\left( \frac{n}{2} \right) + c n$
- Base case: $T(1) = T(0) = d$ (some constant)

Each level in the recursion tree does $c n$ work.
- Total work: $c n \cdot \log n$

**Lemma**
Any algorithm with recurrence $T(n) = 2 T\left( \frac{n}{2} \right) + O(n)$ has a running time of $O(n \log n)$.

## Different ways of showing Runtime

**Proof techniques**
- Unrolling: Expanding the recurrence relations to find a pattern
- Substitution: Assuming a solution and proving it holds

**Unrolling the Recurrence**
- Work in level *j* per node: $$c \cdot \frac{n}{2^j}$$
- Total work per level i: $$c \cdot \frac{n}{2^j} \cdot 2^j = c\cdot n$$
- Number of levels:$$\log_2n$$
- Total work
$$
T(n) \leq \sum_{j=0}^{\log_2(n)} cn = cn\log_2n
$$

**Geometric Progression**
- **General Form**: $$\sum_{j=0}^{N} q^j$$
- **Cases**:
    - If $q = 1$: $$\sum_{j=0}^{N} 1^j = N+1$$
    - If $q < 1$: $$\sum_{j=0}^{N} q^j = \frac{1 - q^{N+1}}{1 - q} \approx O(1)$$
    - If $q > 1$: $$\sum_{j=0}^{N} q^j = \frac{q^{N+1} - 1}{q - 1} \approx O(q^N)$$

## Unrolling examples
-  $T(n)\leq 4T(n/2) + n$
	- work per layer: $4^j$
	- work per node: $cn/2^j$ 
	- layers: $log_2(n)$
$$T(n)= cn \sum_{j=0}^{log_2(n)} \frac{4^j}{2^j} =\frac{cn\cdot n^{log_2(4)}}{n^{log_2(2)}} =\frac{cn^3}{n}= cn^2$$
	- $O(n^2)$
-  $T(n)\leq 4T(n/2) + n^2$
	- work per layer: $4^j$
	- work per node: $c(n/2^j)^2$
	- layers: $log_2(n)$
	- $T(n)= cn^2 \sum_{j=0}^{log_2(n)} 1 = cn^2log_2(n)$
	- $O(n^2log(n))$
-  $T(n)\leq 5T(n/2) + n$
	- work per layer: $5^j$
	- work per node: $cn/2^j$
	- layers: $log_2(n)$
	- $T(n)= cn \sum_{j=0}^{log_2(n)} (5/2)^j = cn^{log_2(5)}$
	- $O(n^{log_2(5)})$
-  $T(n)\leq 5T(n/2) + n^2$
	- work per layer: $5^j$
	- work per node: $c(n/2^j)^2$
	- layers: $log_2(n)$
$$T(n)= cn^2 \sum_{j=0}^{log_2(n)} (5/4)^j = \frac{cn^2n^{log_2(5)}}{n^{log_2(4)}} = cn^{log_2(5)}$$
	- $O(n^{log_2(5)})$
-  $T(n)\leq 10T(n/3) + O(n\sqrt{n}) +13$ 
	- work per layer: $10^j$
	- work per node: $c(n/3^j)^{1.5} + 13$
	- layers: $log_3(n)$
$$
T(n)= cn^{1.5} \sum_{j=0}^{log_3(n)} (10/3^{1.5})^j = \frac{cn^{1.5}n^{log_3(10)}}{n^{log_3(3^{1.5})}} = \frac{cn^{1.5}n^{log_3(5)}}{n^{1.5\cdot log_3(3)}} = cn^{log_3(10)}
$$

	- $O(n^{log_3(10)})$

**Example visually**
![[Pasted image 20241204195831.png]]


## Substitution
- **Base Case**: Verify the inequality for the smallest n (adjust c if necessary).
- **Inductive Hypothesis**: Assume the inequality holds for all $m < n$.
- **Inductive Step**:
    - Substitute the inductive hypothesis into the recurrence relation.
    - Simplify the expression, using logarithmic identities if needed.
    - Show that the inequality holds for n.


![[Pasted image 20241204204340.png]]
Then in the end we have shown that the inequality holds for n

## Masters Theorem
![[Pasted image 20241204205817.png]]
- work per layer: $a^j$
- work per node: $c\cdot(\frac{n}{b^j})^d$ 
- layers: $log_b(n)$
- $T(n) = \sum^{log_bn}_{j=0} c\cdot a^j (\frac{n}{b^j})^d$ = $cn^d \sum^{log_b(n)} (\frac{a}{b^d})^j$ 

- The three cases
	- nice thing to note is that $n^d$ is the cost of the work done outside the recursive calls, we can call it f
	Where each case represent if q is under, equal or over 1
	$O(n^d), d>log_b(a)$
	- Here the sum becomes $O(1)$ and therefore is not included, and f will completely dominate
	$O(n^dlogn), d=log_b(a)$
	- Here the sum is $O(N)$ where $N = log_b(n)$ . f grows with an equal factor as recursive contribution. 
	$O(n^{log_ba}), d < log_ba$
	- Here the sum will originally be $a^N$ as $N = log_b(n)$ which will make $O(n^{log_b(a)})$. Here f will grow slower than the recursive contribution and thus the recursive part will be the dominant part.



---

# Integer Multiplication
*In: A, B*
*Out: A times B*

Assume that |x|=|y| is a power of 2.

$|x|=|y| \in \{0,1,2,4,8,16\}$

$x = 123 \Rightarrow 0123$

$x = x_1 \cdot 2^{n/2} + x_0$

It is not multiplying, but it is only bit-shifting. It is the same as adding 0s at the end of the n

$x\cdot y = (x_1 \cdot^{n/2} + y_0)\cdot(y_1 \cdot^{n/2} + y_0)$
$2^nx_1y_1 + 2^{n/2}(x_1y_0 + x_0y_1) +x_0y_0$ 


**Karatsuba's algorithm (1962)**
$P = (x_1 + x_0)(y_1 + y_0)$
	$= x_1y_1+ x_1y_0 + x_0y_1 + x_0y_0$
	$A = x_1y_1$
	$B = x_0y_0$
  
$= x_1y_1+ x_1y_0 + x_0y_1 + x_0y_0$

$A = x_1y_1$
$P = A + x_1y_0 + x_0y_1 + B$
$P - A - B = x_1y_0 + x_0y_1 = S$
$2^nA+ n^{n/2}(x_1y_0 + x_0y_1) + B$

$T(n) \leq 3T(n/2)+O(n)$
$O(n^{log_23}) = O(n^{1.59})$



# Counting Inversions
*in: list of integers L*
*out: number of pairs that are inverted i, j s.l i< j ,b but L\[i] > L\[j]*

Social choice theory
Clustering on counting inversions

**Observe**
- We can have $\Omega (n^2)$ many inversions
**Corollary**
- counts using count +=1
- it runs in the $\Omega (n^2)$.

```psudo
Count(L):
	inv_1 = count(L_1)
	inv_2 = count(L_2)
	count(L_1, L_2)	
```

Key Idea:
- Divide: Split the list into two halves.
- Conquer: Recursively sort and count inversions in each half.
- Combine: Merge the two sorted halves and count split inversions (inversions where one element is in the left half and the other is in the right half).
	- 

```python
def sort_and_count(L):
    if len(L) <= 1:
        return 0, L
    else:
        mid = len(L) // 2
        inv_left, left = sort_and_count(L[:mid])
        inv_right, right = sort_and_count(L[mid:])
        inv_split, merged = merge_and_count(left, right)
        total_inv = inv_left + inv_right + inv_split
        return total_inv, merged

def merge_and_count(left, right):
    i = j = inv_count = 0
    merged = []
    while i < len(left) and j < len(right):
        if left[i] <= right[j]:
            merged.append(left[i])
            i += 1
        else:
            merged.append(right[j])
            inv_count += len(left) - i  # Count inversions
            j += 1
    # Append any remaining elements
    merged.extend(left[i:])
    merged.extend(right[j:])
    return inv_count, merged

```


# Closest Pair Problem

_Input_: A set of points in the plane

_Output_: A pair of points with the minimum distance between them

- We aim to find the closest pair of points among a set of points in the plane using a divide and conquer approach.

### Algorithm Overview

1. **Divide**:
    - Split the set of points P into two halves L and R by a vertical line (e.g., median x-coordinate).
2. **Conquer**:
    - Recursively find the closest pair in L, denoted $\delta_L$.
    - Recursively find the closest pair in R, denoted $\delta_R$.
3. **Combine**:
    - Let $\delta = \min(\delta_L, \delta_R)$.
    - Find the closest pair across L and R that may be closer than $\delta$.
    - Create a strip S of points within distance $\delta$ from the dividing line.
    - Sort S by y-coordinate.
    - For each point $s_i$â€‹ in S, check for closer pairs among subsequent points in S.

## Pseudocode
```python
def closest_pair(P):
    if len(P) <= 3:
        return brute_force_closest_pair(P)
    else:
        # Assume P is sorted by x-coordinate
        mid = len(P) // 2
        Q = P[:mid]
        R = P[mid:]
        delta_L = closest_pair(Q)
        delta_R = closest_pair(R)
        delta = min(delta_L, delta_R)
        # Create strip of points within delta of the dividing line
        x_bar = P[mid].x
        S = [point for point in P if abs(point.x - x_bar) < delta]
        # Sort strip by y-coordinate
        S.sort(key=lambda point: point.y)
        # Find the closest pair in strip S
        delta_strip = delta
        for i in range(len(S)):
            for j in range(i+1, min(i+16, len(S))):
                if (S[j].y - S[i].y) >= delta_strip:
                    break
                d = distance(S[i], S[j])
                if d < delta_strip:
                    delta_strip = d
        return min(delta, delta_strip)
```

#### Explanation
- **Divide Step**:
    - The set P is divided into two halves L and R based on x-coordinates.
    - This creates a vertical dividing line $x^*$.
- **Conquer Step**:
    - Recursively find the smallest distance $\delta_L$ in L.
    - Recursively find the smallest distance $\delta_R$ in R.
    - Set $\delta = \min(\delta_L, \delta_R)$.
- **Combine Step**:
    - **Strip Creation**:
        - Points within distance $\delta$ from $x^*$ are included in strip S.
        - Formally, $S = \{ p \in P : |p.x - x_{\text{mid}}| < \delta \}$.
    - **Sorting**:
        - Sort S based on y-coordinates to optimize the search for close pairs.
    - **Comparing Points** in S:
	    - For each point $s_i$â€‹ in S:
	        - Compare $s_i$ with subsequent points $s_j$â€‹ where $j>i$.
				- so you don't compare 2 points twice
        - Stop comparisons when:
            - The difference in y-coordinate $\mid s_j.y-s_i.y\mid \geq \delta$
            - This is because if the y-distance is greater than or equal to $\delta$, the Euclidean distance cannot be less than $\delta$.
    - Maximum Number of Comparisons:
	     - It can be shown that for each $s_i$â€‹, we need to check **at most 7** subsequent points in S. 

#### Proof of the Claim
**Lemma**: In a strip of width $2\delta$, after sorting points by y-coordinate, each point $s_i$â€‹ needs to be compared with at most 15 subsequent points $s_{i+1}â€‹,s_{i+2}â€‹,\dots,s_{i+15}â€‹$.

### Proof:
1. **Grid Argument**:
    - Overlay a grid of squares of size $\delta/2 \times \delta/2$ over the strip S.
    - Each square can contain at most one point from S.
        - **Reason**: If a square contained two or more points, they would be within a distance less than or equal to $\delta/2 \times \sqrt{2} < \delta$, where both points are either on left side or right side, contradicting that $\delta$ is the minimum distance found so far.
			-  $\delta = \min(\delta_L, \delta_R)$
1. **Neighboring Squares**:
	- Each point can only be within $\delta$ distance of points in its own square and the **15 neighboring squares**.
	- **Due to the ordering by y-coordinate**, points that are more than 15 positions apart in S are in non-adjacent squares vertically.
2. **Conclusion**:
    - **Therefore**, for any point $s_i$â€‹ in S, we only need to **check the next 15 points** in S to find any pair that may be closer than $\delta$.
    
	- **Explanation**:
	    - Since each square in the grid contains **at most one point** from SSS, and considering the arrangement of squares:
	        - Each square has adjacent squares horizontally and vertically.
	        - A point $s_i$â€‹ can have points in up to **15** neighboring squares.
	    - **Implications for Point Comparisons**:
	        - Each point $s_i$â€‹ only needs to compare with points in these adjacent squares.
	        - Given that S is sorted by y-coordinate, these adjacent points correspond to the next **15 points** in the list S.
	- **Why Points Beyond the Next 15 Can Be Ignored**:
	    - Any point $s_j$â€‹ that is more than 15 positions ahead of $s_i$â€‹ in S must be located in a square that is **not adjacent** to the square containing $s_i$â€‹.
	    - The vertical distance between $s_i$â€‹ and $s_j$â€‹ is at least $3 \times (\delta/2) = \frac{3\delta}{2} > \delta$.
	   
![[Pasted image 20241204223142.png]]
#### Final Algorithm
- **Time Complexity**:
    - The algorithm runs in $O(n \log n)$ time.
        - Sorting points initially takes $O(n \log n)$.
        - The recursive steps and the combine step at each level take $O(n)$.
        - There are $O(\log n)$ levels due to the division of the set.
	- Brute force $O(n^2)$
- **Space Complexity**:
    - Requires $O(n)$ extra space for the recursive calls and the strip S.