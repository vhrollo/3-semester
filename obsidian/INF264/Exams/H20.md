# F1e 
Every model uses inferences to make an assumptions, if we do not take assumptions and try to generalize, we will instead be more likely to overfit to the training data.

An more likely scenario why Alice is getting bad results, may be due to underfitting. The linear model may not be able to catch the more complex structure of the data, which will lead to bad results.

# F1f
It seems like Alice's model will not, since she is using a linear model, which is simplifying the assumptions, i.e. underfitting. Something which could lead to high bias.

Bob's model could be too complex, and therefore overfitting. Something which lead to high variance. Generally when using a big neural network as this we do not want a small data set. The more we have, the more likely we are to capture the underlying structure.

# F1b
The no free lunch theorem applies here, where If A is better than B on a dataset D, then there should also exist another dataset where B is better than A. What can happen, is that the neural network is more likely to do it better in real world scenarios.

# M2c
By the looks of it, it is as Bob says; this implementation is working correctly. We can see this as K-means will assume the cluster is circular. Another clustering which would work better in this case could be density, hierarchical, and maybe even GMMs could work.

# M2h
The data is imbalanced, and therefore Bob is blind to high numbers compared to Alice. She predicted relatively good on both classes, and if we would used another score such as weighted f1 or even weighted recall, to check how, and if proportional weighing is not enough, we could use balanced_accuracy, which will weigh each recall equally.

# E3a
The principal vectors has to be orthogonal, the length of the vector represent the variance, which we can see is completely wrong. In a real scenario, the red vector would be the longest, the first principal component, and the blue would be shorter, and orthogonal to the red, since the red seems to be in a more or less right position. The rest is correct

# E3b
Neural Networks can be used in supervised learning. An example of this is auto encoders, which will learn how to reduce a set of features down to k many features.

It is not true that it is impossible to approximate complex function suing just one layer of hidden neurons. Just one layer can be an universal approximation function, as long as it has enough neurons in this layer.

When neural networks have a lot of weights they can tend to overfit if we don't do anything about it. 

# G5a
papir

# H4b
papapiojso

# S6a

**1. overfit or not**

Model A:
- A strong sign of overfitting is that the accuracy is very high in training and very low in testing, which this is. This can also be seen visually too.
Model B:
- This seems a to be generalizing pretty well
Model C:
- This also seems to generalize well

**2. My choice**
- I would choose C as of Occam razor, it seems simpler than the ragged B, and it doesn't overfit. It also has the highest accuracy and f1 score on validation data.

**3. unbiased estimate**
- We don't have any data which is unbiased to test with, so therefore this cannot be done
