# F1e 
Every model uses inferences to make an assumptions, if we do not take assumptions and try to generalize, we will instead be more likely to overfit to the training data.

An more likely scenario why Alice is getting bad results, may be due to underfitting. The linear model may not be able to catch the more complex structure of the data, which will lead to bad results.

# F1f
It seems like Alice's model will not, since she is using a linear model, which is simplifying the assumptions, i.e. underfitting. Something which could lead to high bias.

Bob's model could be too complex, and therefore overfitting. Something which lead to high variance. Generally when using a big neural network as this we do not want a small data set. The more we have, the more likely we are to capture the underlying structure.

# F1b
The no free lunch theorem applies here, where If A is better than B on a dataset D, then there should also exist another dataset where B is better than A. What can happen, is that the neural network is more likely to do it better in real world scenarios.

# M2c