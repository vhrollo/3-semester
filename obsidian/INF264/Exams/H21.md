# 1
The alcohol usage seems to be MAR, as they match the age of the person. The salary seems to be MCAR as there isnt anyhing and there is no pattern we could use or infer.

We should impute the values

# 2
No free lunch theorem, if the supervisor things linear model is the most likeli to succeed at this dataset. Generally more complex models has the ability to overfit much easier.

# 3
Vegard just classifies everything as correct, not good, so his F1 score doesn't make sense to use. 
Both are bad, but Bård manages to find 60/300 true labels without classifying a false as true, which is somewhat better. This is not respective to both classes and therfore f1 would be misleading

# 4
Cross-validation would only reduce variance on the performance measure as Halfrid is saying, overfitting can still happen. It will then increase the probability of us choosing a model which is the best model.

# 5 
Matti has no prior knowledge on how Ålesund and Molde is, then the only reasonable thing is a 50/50 is bigger, while Espen he has prior knowledge as he has been to both of the cities before, and therefore can use thatn to come up with the probability that ålesund is more likeli to be bigger based on other factors. Espen and Matti are then Baysianist, and Tommy is a frequentist, and will only use probabilty on randomness and uncertanantiy, while if ålesund is bigger than molde is a fact

# 6 

Task: Recognize authorized people and open the door
Data: Images of all authorized people, and then random non authoarized
Performance measure: if authorized people are positive i would something like weighted f1 score on this is good, since both recall and precision is relevant
- recall we want to see how many of the true ones actually got in, and precision where we check how many of the ones who got in are authorized

# 7 ensamble methods

We choose the majority label. The feature which is gonna be split is not randomly selected from the subset, it will be chosen on which gives us more information gain. bagging can still overfit. It is not true that the boosting will make it unbiased towards the data it trained on. 

No free lunch theorem can be applied, a lot of weaker and unique classifiers can together become a strong classier in this scenario. There is no such thing as a best machine learning algorithm


# 8 gg
did in my book oooo
# 10

