# Idea
- Generate a collection ( ensemble ) of classifiers ( of regressors ). Where each classifier can be weak.
- The final prediction is defined by voting ( or averaging )
- Each weak classifier can be simple with large error; the only requirement is that they are better than random

# Two different approaches:
**Bagging**: reduce variance
**Boosting**: reducing bias


## Reducing Variance

- **Mean and variance of a weighted sum**:
  - Mean:
    $$ E[a_1 \hat{f}_1(x) + a_2 \hat{f}_2(x)] = a_1 E[\hat{f}_1(x)] + a_2 E[\hat{f}_2(x)] $$
  - Variance:
    $$ \text{Var}[a_1 \hat{f}_1(x) + a_2 \hat{f}_2(x)] = a_1^2 \text{Var}[\hat{f}_1(x)] + a_2^2 \text{Var}[\hat{f}_2(x)] + 2 a_1 a_2 \text{Cov}[\hat{f}_1(x), \hat{f}_2(x)] $$

- **Averaging to reduce variance**:
  - Averaged prediction:
    $$ \hat{f}(x) = \frac{1}{T} \sum_{i=1}^T \hat{f}_i(x) $$
  - If models are independent with the same variance:
    $$ \text{Var}[\hat{f}(x)] = \frac{1}{T} \text{Var}[\hat{f}_i(x)] $$

- **Bias and variance with averaging**:
  - If each learner $\hat{f}_i$ has the same bias, then the averaged model $\hat{f}(x)$ has the same bias.
  - **Variance reduction**: Averaging \( T \) independent learners reduces variance by $\frac{1}{T}$.

- **Conclusion**: Averaging ( ensembling ) reduces variance without affecting bias.


**How do we get different predictions, i.e. independent predictions**
- We use different training data
- Different model family
- different learning algorithm
- seed?

Different Data sets is then needed to make different classifiers, and that is where bootstrap sampling is used, also called **B**ootstrap **agg**regat**ting** - bagging.

## Bagging

In bootstrapping, one samples new training set by sampling n points with replacement
- You will on average have 63.2% of the data in each set

**This can be done in a Random Forest** (First assignment)
1. Create bootstrap samples
2. Learn a decision tree for each sub-sample
	- At each node in the tree, choose K features at random and choose the best split among them
3. Combined predictions (majority vote or averaging)

**Properties about bagging**:
- Low bias base models is needed
- Works best with unstable models, with high variance
- Classifiers need to be independent
## Boosting

- **Weak learner** $1/2 + \tau$ accuracy
- **Strong learner** $1 - \tau$ accuracy
A weak learner can be boosted up to a strong learner

You train a sequence of learners so that subsequent models will compensate the errors
Typically, data for the classifiers are adaptively resampled (re-weighted) so that the data points with which the classifiers has had problems will get larger weight

**This begin with equal weights on all points**:
1. Learn a classifier on the weighted data
2. Compute weighted training error
3. Compute "classifier weight",  High if small error low if large error
	- the alpha value, weaker learners have less importance 
4. Update weights for the data points (increase weight if the current classifier made an error, decrease weight if current classifier predicted correctly)

**Formally**
- **Initialize** weights: 
  $$ w_1(i) = \frac{1}{n} \text{ for all } i = 1, \dots, n $$

- **For** each iteration \( t = 1, \dots, T \):
  1. Train a weak learner \( h_t(x) \) using the training data and weights \( w_t(i) \).
  2. Compute \( \epsilon_t \), the weighted training error for \( h_t \):
     $$ \epsilon_t = \sum_{i=1}^n w_t(i) \cdot \mathbf{1}(y_i \neq h_t(x_i)) $$
  3. Compute \( \alpha_t \):
     $$ \alpha_t = \frac{1}{2} \log \frac{1 - \epsilon_t}{\epsilon_t} $$
  4. Update weights:
     $$ w_{t+1}(i) = w_t(i) \cdot \frac{\exp(-\alpha_t y_i h_t(x_i))}{Z_t} $$
     where \( Z_t \) is the normalizing constant to ensure \( w_{t+1} \) sums to 1.

- **Output the final classifier**:
  $$ H(x) = \text{sign} \left( \sum_{t=1}^T \alpha_t h_t(x) \right) $$


Why does this work, each classifier specializes on a particular subset of the training data
Its concentrating on the more difficult data
This can eliminate effects of high bias, of the base learner( unlik bagging )
This can reduce variance
We use weak learners since thay are faster



## Overfitting
- Boosting can overfit if you have too many base learners
- I Bagging can overfit if base models are too complicated/too dependent or there are not enough bootstrap samples



