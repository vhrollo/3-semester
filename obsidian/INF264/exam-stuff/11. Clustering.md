*Used to find intrinsic structure in unlabeled data. Cluster is a set items so that items are similar within a cluster and dissimilar between clusters*

## Types of clusters

**Centroid models**: a cluster is represented by a single mean vector
**Distribution models**: clusters are modeled with statistical distributions
**Connectivity models**: build clusters based on distance connectivity
**Density models**: a cluster is a set of points connected by dense regions
**Gaussian models** a cluster is a Gaussian model which predict how likely a point is to be in that cluster



## K-means
- Represented by a centroid
- We assume that there is exactly k *clusters*
- **Within-cluster variance for cluster \( j \)**:
  $$ \text{Var}[D_j] = \frac{1}{|D_j|} \sum_{x_i \in D_j} \| x_i - \mu_j \|^2, $$
  where $( \mu_j )$ is the mean vector of the points belonging to cluster $( j )$.

- **Weighted within-cluster variance** (by cluster size):
  $$ |D_j| \, \text{Var}[D_j] = \sum_{x_i \in D_j} \| x_i - \mu_j \|^2. $$

+ **Cost function**:
$$ C(D_1, \ldots, D_k) = \sum_{j=1}^{k} |D_j| \, \text{Var}[D_j] $$
+ Finding an partition would be NP-hard, therefore we use heuristics


### Lloyd's algorithm
Pick initial cluster centroids
Repeat until nothing changes
- Assign each data point to a cluster whose centroid is closest to that point
- Update centroid: the new centroid of the cluster is the mean of the data points assigned to that cluster.

The cost function $C(D_1, \ldots, D_k)$ for clustering is defined as:

$$
C(D_1, \ldots, D_k) = \sum_{j=1}^{k} |D_j| \, \text{Var}[D_j]
$$

where:
- $D_j$: Cluster $j$.
- $|D_j|$: Number of points in cluster $j$.
- $\text{Var}[D_j]$: Within-cluster variance for cluster $j$.

This is a lot faster than $k^n$ iterations, but it will converge to a local minimum

Bad initialization can lead to:
- slow convergence
- bad clustering results

**Elbow method**
- sometimes work to find a good representative k
-  you increase k's until variance is not reduced significantly per k

**Silhouette coefficient**
- Measures similar a point is to its own clusters compared to other clusters
- basically smallest average distance to any other clusters points - average distance to the points in its own cluster
- then normalize this onto max(these to averages)

**Limitations of k-means**
- needs a k
- sensitive to outliers
	- k-medians can b used
- works good on round shapes
- prone to a local minima

## Gaussian Mixture Models (GMMs)

A **Gaussian Mixture Model** (GMM) is a probabilistic model that assumes data is generated from a mixture of several Gaussian distributions, each representing a cluster in the data.

1. **Model Density Function**:
   - For a GMM with $k$ components, the probability density function is given by:

     $$
     p(x) = \sum_{j=1}^{k} \pi_j \, N(x \,|\, \mu_j, \Sigma_j)
     $$

   - Here:
     - $x$ is the data point.
     - $N(x \,|\, \mu_j, \Sigma_j)$ represents the **Gaussian (normal) distribution** for component $j$, with mean $\mu_j$ and covariance $\Sigma_j$.

2. **Components (Clusters)**:
   - Each **component** $N(x \,|\, \mu_j, \Sigma_j)$ represents a cluster in the data, characterized by its own mean $\mu_j$ and covariance $\Sigma_j$. This allows each cluster to have a unique shape, position, and orientation in the feature space.

3. **Mixing Coefficients**:
   - $\pi_j$ are the **mixing coefficients** that represent the weight of each component in the overall mixture model. They determine the proportion of data points that belong to each component.
   - These mixing coefficients satisfy two conditions:
     - They sum to 1:

       $$
       \sum_{j=1}^{k} \pi_j = 1
       $$
     - They lie within the range $0 \leq \pi_j \leq 1$, meaning each coefficient is a probability.

---
- **GMMs** model data as a mixture of Gaussian distributions, with each Gaussian representing a cluster.
- Each component has its own **mean** $\mu_j$ and **covariance** $\Sigma_j$, defining the location and spread of the cluster.
- The **mixing coefficients** $\pi_j$ indicate the importance of each component and ensure the model represents a valid probability distribution.

This model is commonly used in **clustering** and **density estimation** tasks, where the goal is to find underlying group structures in the data.

**Eigenvectors and eigenvalues** of each covariance matrix $\Sigma_j$​ define the **shape** (oblong or spherical) and **orientation** of each Gaussian component, not the summation itself.
$$ \Sigma_j = \begin{bmatrix} \sigma_{11} & \sigma_{12} \\ \sigma_{12} & \sigma_{22} \end{bmatrix} $$

## Hierarchical clustering
*Produces nested clusters organized in a hierarchy tree*

**Convenient**:
- No need to decide the number of clusters
- No need to worry about initialization
- We need to measure dissimilarities $d_{ij}$ between points
- also need to specify linkage, a way to measure dissimilarity


**Agglomerative**:
- bottom up
- start with n clusters with one point in each, then merge then until we have one cluster
-  as long as there is more than one cluster
	- merge to clusters that are closest to each other
	- this is done using linkage
	- similarity score ( or distance) is usually pre-computed and stored in a matrix
**Divisive**:
+ top down
- goes from one cluster to n points

To get a “flat” clustering, one can “cut” the hierarchy tree at an appropriate level
## Linkage
*Linage functions define how the distance between clusters are computed*

**Single linkage**
- Distance between neighbors
- elongated clusters
**Complete linkage**
- distance between farthest neighbors
- compact clusters
**Average linkage**:
- average between all pairs of points
- compromise between the two above
**Centroid**
- distance between mean of the clusters
**Ward**
- the change of the total distance between mean of clusters merged vs. total mean distance with them two apart

## Density-based clustering
- Group together points that are closely packed together, points with many neighbors
- Points that lie in low-density is outliers
- Density-Based Spatial Clustering of Application with Noise (DBSCAN)

## DBSCAN Algorithm

1. **Set Parameters**: Define $\epsilon$ (distance threshold) and `minPoints` (minimum points for a cluster).
2. **Select a Point**: Pick an unvisited point and find all points within distance $\epsilon$.
3. **Form a Cluster or Label as Noise**:
   - If there are at least `minPoints` neighbors, start a cluster.
   - If not, label the point as noise.
4. **Expand Cluster**: Add all reachable points within $\epsilon$ of cluster points.
5. **Repeat** until all points are visited and clustered or labeled as noise.
