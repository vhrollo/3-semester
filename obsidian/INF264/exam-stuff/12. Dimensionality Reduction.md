*In higher dimensions nearest neighbor will be far away. And at high dimensions almost all distances are the same*

## basic
- It makes it easier to understand the data
- Models for data with lots of features but only a handful of samples is likely to overfit
- curse of dimensionality
- de-noising
- preprocessing for supervised learners

---

## Feature selection
*variable selection*

- We would then keep features of high variance, that also have low covariance
- We remove constant features, and redundant, f.ex dependent features

- embedded methods
	- decision trees do it implicitly
	- Linear models with $L_1$ regularization

- Unsupervised
	- redundant features with correlation test
	- remove features  with low variance, variance threshold

---
## Feature extraction
*Create a small set of new variables that are informative, and they are learned from the data*

**Vector projecting**
- Given a line vector, we can project them down a dimension


---
### PCA - principal component analysis
*new dimensions are linear combinations of the original ones*
- Learn most interesting directions
	-  a new set of basis functions
- Represent them using these directions
	- change of basis
- Throw away uninteresting directions

- The goal is to maximize the variance in an m-dimension

**Pros**
- fast and simple
- often works well
**weakness**
- scaling affects results
- relies on linearity assumption
- maximum variance is not always the most predictive ones

##### 1. Projection and Variance
- **Projection**: Each data point $x_i$ is projected onto a scalar $z_i = u_1^T x_i$.
- **Mean of Projected Data**:
  $$
  \bar{z} = u_1^T \bar{x}
  $$
- **Variance of Projected Data**:
  $$
  \text{Var}(z) = u_1^T S u_1
  $$
  where $S = \frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})(x_i - \bar{x})^T$ is the covariance matrix.

##### 2. Maximizing Variance
- **Objective**: Maximize $u_1^T S u_1$ subject to $u_1^T u_1 = 1$.

##### 3. Solving with Lagrange Multipliers
- **Lagrangian**:
  $$
  L(u_1) = u_1^T S u_1 + \lambda_1 (1 - u_1^T u_1)
  $$
- **Solution**: $S u_1 = \lambda_1 u_1$, where $u_1$ is the eigenvector with the largest eigenvalue $\lambda_1$.

##### 4. Finding More Principal Components
- Additional components are found by choosing directions that maximize variance and are orthogonal.
- **Eigenvectors**: Collect eigenvectors corresponding to the top $m$ eigenvalues into $W$:
  $$
  W = [u_1 \; u_2 \; \ldots \; u_m]
  $$

##### 5. Solving PCA in Practice
1. **Center the Data**: $\hat{x}_i = x_i - \bar{x}$
2. **Covariance Matrix**:
   $$
   S = \frac{1}{n} \hat{X}^T \hat{X}
   $$
3. **Eigen Decomposition**: Find eigenvalues and eigenvectors of $S$.
4. **Select Top $m$ Components** to form $W$.
5. **Project Data**:
   $$
   Z = \hat{X} W
   $$

**It is computed like this**
1. **Original Data Matrix** Let $X_{\text{std}}$ be the standardized data matrix of shape $n \times 4$, with $n$ samples and 4 features: $$ X_{\text{std}} = \begin{bmatrix} x_{11} & x_{12} & x_{13} & x_{14} \\ \vdots & \vdots & \vdots & \vdots \\ x_{n1} & x_{n2} & x_{n3} & x_{n4} \end{bmatrix} $$ 2. **Transformation Matrix** The transformation matrix $W$ is composed of the top two eigenvectors, with shape $4 \times 2$: $$ W = \begin{bmatrix} w_{11} & w_{12} \\ \vdots & \vdots \\ w_{41} & w_{42} \end{bmatrix} $$ 3. **Projection** Project $X_{\text{std}}$ onto 2 dimensions by computing $Y = X_{\text{std}} \cdot W$, giving $Y$ shape $n \times 2$: $$ Y = \begin{bmatrix} y_{11} & y_{12} \\ \vdots & \vdots \\ y_{n1} & y_{n2} \end{bmatrix} $$

##### A lil about the eigenvector and its eigenvalue

**First Principal Component**: The eigenvector $u_1$ associated with the largest eigenvalue $\lambda_1$ defines the **first principal component**. This component is the direction along which the data varies the most.
- The eigenvalue $\lambda_1$ represents the **amount of variance** in the data along the direction of its corresponding eigenvector $u_1$.

If you choose to project onto the **top k eigenvectors** (principal components), then each data point is represented in a **k-dimensional space**.

**Reconstructing the data**:
- $X \approx ZW^T +\overline{X}$   
- If you know the direction of the "dimension" you reduced to, you can represent this in a higher dimension, by representing this dim as a line in higher space

### Some non linear methods
- kernel pca
- independent component analysis
- locally linear embedding
- t-SNE


### Auto encoders
*Neural networks that are used to learn low-dimensional
representation of data*

- Maps an input x to an output r (called reconstruction) through an internal representation h
- Model is forced to prioritize the most important aspects of the input when you lower the dimensions of h
- $L(x, g(f (x)))$ 