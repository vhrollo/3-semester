
# Training data
*Quantity and quality give an upper bound for what can be learned*

**Signal vs. noise**
This cannot be done without signal ($\approx$ features x contain information about y)
- $y=f(x)+\epsilon$

Then more data gives you the ability to detect weaker signals buried within the noise


# Bias
*systemic error, which is caused by unrealistic assumptions*

**Informal Def.** (bias):
	The signal in the training data does not equal the true ( or desired ) signal

**Sources of bias**:
- Sample bias
- Human bias
- Measurement bias


### Fairness
 *Biased behavior, these can emerge due to historical inequalities*

### Manipulation of the data
*Altering the data to influence a systems behavior*

### Missing data
**Missing completely at random (MCAR)**
- The probability of missingness does note depend on the value of the missing variable or any other variable
**Missing at random (MAR)**
- The probability of missingness depend on values of other features but not on the value of missing feature itself
**Not Missing at random (NMAR) or Missing Not at Random (MNAR)**
- The probability of missingness depends on the value of the variable itself

**This can be handled in two ways**:
- Using common sense
- Statistical tests
	- Create binary variable (missing data indicator)
	- Test if the data points with w/ differ from points w/o missing values
	- If yes then its not MCAR

**How to deal with this**:
- Remove rows with missing values
- Impute missing value
	- replace the missing value with the mean of that particular feature
	- maintains the sample size
	- correlation
- advanced imputation methods
	- use values of other features to predict the missing value

**Imbalanced data**
- Majority class will get prioritized and that class will get high accuracy
	- which means that *high accuracy => good performance* is not true
	- Scores such as F1, precision and recall might be more appropriate

**To deal with this**
- oversample (re-sampling)
- Synthetic samples
	- **SMOTE**
		- synthetic images between points
	- **Focal loss**
		- Putting more weight on difficult samples

**Sanity check**
- do not blindly trust the numbers
- look at the data
- use baselines
- use DK if possible