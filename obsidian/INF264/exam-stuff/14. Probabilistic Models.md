
# Probabilistic modeling
*systemic approach to act in a pool of information with uncertainty. These models know when they don't know*


## Uncertainty
**Aleatory uncertainty**
- Due to randomness
- We are not able to obtain observations which can reduce this uncertainty

**Epistemic uncertainty**
- due to lack of knowledge
- the uncertainty can be reduced by gathering more observations
- two observers may have different epistemic uncertainty

## Probability interpretation
**Frequentist objective** 
- Frequencies from repetition of experiments
- P(A) is the proportions of times A occurs to be true
- handles aleatory uncertainty

**Bayesian**
- A are propositions and P(A) is a degree of belief in A being true
- Uses prior knowledge, which refers to any information, assumptions, or belief we have about a parameter, before observing any new data
- handles both aleatory and epistemic uncertainty


**Bayes' theorem**
$$
P(A \mid B) = \frac{P(B \mid A) \cdot P(A)}{P(B)}
$$
- $P(A \mid B)$ is the **posterior probability**
- $P(B \mid A)$ is the the **likelihood** of B based on A being true
- $P(A)$ is the **prior** probability, before observing evidence
- $P(B)$ is the **marginal probability** of B
	- normalizing constant?

### "standard" modeling
*this is where we have selected a model which we justify be some metric to be the "best"*
- A model that can correspond to the maximum likelihood $argmax_\Theta P(D\mid \Theta)$ 

### Bayesian modeling
*instead of returning a single model, it will return a probability distribution of models*

- It will summarize the knowledge in a prior distribution $P(\Theta)$ 
- It will update the belief based on observed data D
- Output posterior distribution $P(\Theta\mid D)$ 

### Terminology

**Likelihood** $P(D \mid \Theta)$, is the probability of observing the particular data *D* given the data generated from a model with parameters $\Theta$
- based on f.ex loss
**Prior** $P(\Theta)$ is the probability distribution of the parameters before seeing data
- does not depend on the data
- use of other information
- epistemic uncertainty
- "Regularization" / inductive bias
**Posterior** $P(\Theta \mid D)$ is the probability of the parameters given the data
- Probability of a model after seeing the data
- Our posterior believes are updated based on the data we see



# Bayesian Prediction

The Bayesian inference returns a posterior distribution over model parameters. So if you want to predict, you need to take a weighted average of predictions 
$$
P(y \mid \mathbf{x}, D) = \int P(y \mid \mathbf{x}, \theta) \, P(\theta \mid D) \, d\theta
$$
- $P(y\mid x, D)$ is the **predictive distribution** for y, where the outcome we are interested in predicting given input *x* and past distribution *D*
- $\int P(y \mid \mathbf{x}, \theta) \, P(\theta \mid D) \, d\theta$ 
	- The integral sums over all possible values of $\theta$, taking into account both the likelihood $P(y \mid \mathbf{x}, \theta)$ and the posterior $P(\theta \mid D)$. This means our prediction for $y$ considers every possible model configuration, weighted by how plausible each configuration is given the data.
	- this will give the **predictive distribution** 
	- basically the volume under the graph of $P(y \mid \mathbf{x}, \theta) \, P(\theta \mid D)$
- Both $P(y \mid \mathbf{x}, \theta)$ and $P(\theta \mid D)$ are *probability distributions*, meaning they are normalized so that their total area (or volume) under the curve is equal to 1.

- Then the volume of the predictive distribution will also be 1, and the height of x, will represent the probability y

# Bayesian Linear Regression
Here, $\mathbf{w}$ is the vector of weights or coefficients we want to estimate, and $\epsilon_i$ represents the error or noise in the observation.
$$
y_i = w^Tx_i + \epsilon_i
$$
Assuming that errors are independent and follow Gaussian distribution, that is, $\epsilon_i \sim N(0, \sigma^2)$, the likelihood of our linear model is:
$$
P(D \mid \mathbf{w}) = \ell(\mathbf{w}) = \prod_{i=1}^n \mathcal{N}(y_i - (\mathbf{w}^T \mathbf{x}_i) \mid 0, \beta^{-1}),
$$
So, $\beta = \frac{1}{\sigma^2}$ and is called the precision. Basically $\beta^{-1}$ is just a quirky way of writing $\sigma^2$. So we are just multiplying each likelihood together, to get a likelihood over all the observed points.

We want to find an optimal w for this that's why we are max likeli.. or MSE

Since we want to maximize the likelihood, we could also MSE, as it is equivalent to the likelihood function:
$$
\sum_{i=1}^n \epsilon_i^2 = \sum_{i=1}^n (y_i - \mathbf{w}^T \mathbf{x}_i)^2
$$
The posterior distribution is proportional to the product of the likelihood and the prior
$$P(\mathbf{w} \mid D) \propto P(D \mid \mathbf{w}) P(\mathbf{w})$$


- **Precision parameter $\beta$ in the likelihood**:
  $$ 
  P(D \mid \mathbf{w}) = \prod_{i=1}^n \mathcal{N}(y_i - \mathbf{w}^T \mathbf{x}_i \mid 0, \beta^{-1})
  $$
  - $\beta$ controls the noise level in the data: a higher $\beta$ (higher precision, lower variance) implies less noise, so the model will fit the data more closely.
  - A lower $\beta$ allows for more noise, meaning the model expects the data points to vary more around the linear predictions.

- **Precision parameter $\alpha$ in the prior on $\mathbf{w}$**:
  $$ 
  P(\mathbf{w} \mid \alpha) = \mathcal{N}(\mathbf{w} \mid \mathbf{0}, \alpha^{-1} \mathbf{I})
  $$
  - $\alpha$ controls the prior belief about the size of the weights $\mathbf{w}$: a higher $\alpha$ (higher precision) shrinks $\mathbf{w}$ towards zero, acting as regularization and reducing model complexity.
  - A lower $\alpha$ allows for larger values in $\mathbf{w}$, making the model more flexible to fit the data.

- **Summary of hyperparameters**:
  $$ 
  \Gamma = \{\alpha, \beta\}
  $$
  - Together, $\alpha$ and $\beta$ control the model's flexibility and fit, balancing data fit (likelihood) and regularization (prior).


After some computations, we get the posterior
$$
P(\mathbf{w} \mid \Gamma, D) = \mathcal{N}(\mathbf{w} \mid \mathbf{m}, \mathbf{S}),
$$
where
$$
\mathbf{S} = \left( \alpha \mathbf{I} + \beta \sum_{i=1}^n \mathbf{x}_i \mathbf{x}_i^T \right)^{-1},
\quad \mathbf{m} = \beta \mathbf{S} \sum_{i=1}^n y_i \mathbf{x}_i.
$$

### Predicting y given x in Bayesian Linear Regression
- **Mean Prediction** $\tilde{y}$:
  $$
  \tilde{y} = \int \mathbf{w}^T \mathbf{x} \times P(\mathbf{w} \mid \Gamma, D) \, d\mathbf{w} = \mathbf{m}^T \mathbf{x}
  $$
  - This is the expected prediction by integrating over all possible values of $\mathbf{w}$, weighted by the posterior $P(\mathbf{w} \mid \Gamma, D)$.
  - Result: $\mathbf{m}^T \mathbf{x}$, where $\mathbf{m}$ is the posterior mean of $\mathbf{w}$.

- **Posterior Predictive Distribution**:
  The distribution of possible $y$ values given $\mathbf{x}$, accounting for uncertainty in both data and model parameters.
  $$
  P(y \mid \mathbf{x}, D, \Gamma) = \int \mathcal{N}(y \mid \mathbf{w}^T \mathbf{x}, \beta^{-1}) \, P(\mathbf{w} \mid D, \Gamma) \, d\mathbf{w}
  $$
  
  Substituting the posterior for $\mathbf{w}$:
  $$
  = \int \mathcal{N}(y \mid \mathbf{w}^T \mathbf{x}, \beta^{-1}) \, \mathcal{N}(\mathbf{w} \mid \mathbf{m}, \mathbf{S}) \, d\mathbf{w}
  $$
  Resulting in:
  $$
  = \mathcal{N}(y \mid \mathbf{m}^T \mathbf{x}, \beta^{-1} + \mathbf{x}^T \mathbf{S} \mathbf{x})
  $$

  - **Mean**: $\mathbf{m}^T \mathbf{x}$, the same as the mean prediction.
  - **Variance**: $\beta^{-1} + \mathbf{x}^T \mathbf{S} \mathbf{x}$, combining:
    - $\beta^{-1}$: Noise in the observations.
    - $\mathbf{x}^T \mathbf{S} \mathbf{x}$: Uncertainty in the model parameters $\mathbf{w}$.

- **Summary**: The posterior predictive distribution $P(y \mid \mathbf{x}, D, \Gamma)$ is a Gaussian distribution with mean $\mathbf{m}^T \mathbf{x}$ and variance $\beta^{-1} + \mathbf{x}^T \mathbf{S} \mathbf{x}$, reflecting both observation noise and parameter uncertainty.


# Naive Bayes Classifier
*The Naive Bayes classifier is a **probabilistic model** that predicts the class of a data point based on the probability of each class given the features*

*It applies **Bayesâ€™ theorem** to compute the posterior probability of each class given the observed features and then assigns the data point to the class with the highest posterior probability.*

- Naive assumption: all features are conditionally independent given the class
$$P(C, x_1, x_2, ..., x_d) = P(C)\prod^d_{i=1}P(x_i|C)$$
- Calculate through joint probability since everything is independent 

So when calculating the probability of a class given the features
$$P(C \mid x_1, x_2, \ldots, x_d) \propto P(C) \prod_{i=1}^d P(x_i \mid C)$$
So when predicting, you take the max of all of these:
$$
C^* = \arg \max_C P(C \mid x_1, x_2, \ldots, x_d)
$$