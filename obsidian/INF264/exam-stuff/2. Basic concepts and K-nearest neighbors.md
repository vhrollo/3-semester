# Terminology
**Task** is what the end user actually wants to do
**Model** is a (hopefully good) solution to the task
**Model family** (or hypothesis space) is a set of models from which the model is chosen
**Machine learning algorithm** produces a model based on data
**Features** are used to represent objects

**Classification** predicts the label of the data
**Regression** predicts a label from a set of continuous choices

### Inductive Bias
*induction: from specific cases to general rules*

The inductive bias of a learning algorithm is the set of assumptions that the learner uses to predict outputs given inputs that it has not encountered.

So then an model without inductive bias cannot generalize, only memorize training data

### Evaluating generalization
- Partition your data into two sets (training and validation sets) BEFORE you take a look on it
- Use the training set to train your models
- Use the trained models to make predictions about examples in the validation set
- Compare predicted labels to true labels and compute your performance measure
- Choose the model with better performance on the validation set

**Terminology**
- Validation data: Unseen data that is used to select a model
- Test data: Unseen data that is used to evaluate performance of the selected model

# K-nearest neighbor
*Find k points that are most similar to the test point. Let them vote for the class label*
- Large k reduces the effect of noise; small k increases flexibility
- For binary classification, an odd k avoids ties


**Overfitting** A model performs well on the training data but generalizes poorly
- Can be bc of noise, too complex model, too little data


### Distance measures
*Can be used to compute the similarity or dissimilarity between objects*

Common distance measures:
### Common Distance Measures
- **Euclidean distance**: 
  $$
  d(\mathbf{x}, \mathbf{z}) = \sqrt{\sum_i (x_i - z_i)^2}
  $$
- **Manhattan distance**: 
$$
  d(\mathbf{x}, \mathbf{z}) = \sum_i |x_i - z_i|
  $$
- **L\(_p\)-norm**:
$$
  \|\mathbf{x} - \mathbf{z}\|_p = \left( \sum_i |x_i - z_i|^p \right)^{1/p}
  $$


### Why use this?
- It requires no assumption about the data
- Versatile, simple, and often gives goof performance
- With enough training points, can approximate any decision boundary
	- universal approximation theorem

### Cons
- High memory requirement and slow
- Sensitive to irrelevant features and the scale of the data
- Curse of dimensionality