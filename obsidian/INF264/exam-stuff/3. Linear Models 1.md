# Linear Regression and Classification

## Linear Regression
*Simple and computationally convenient*

**Univariate regression**: 
- Using only one feature x to predict y using function f, where there can exist an residual $\epsilon$:
$$
y = f(x) + \epsilon
$$
- The f is just an linear function in this case:
$$
f(x) = w^T \cdot x + b
$$
- We will use Empirical Risk Minimization (ERM): - *minimize loss on known dataset, i.e. training set*
	- Loss function : $L(y, f(x)) = (y - f(x))^2$, where $E(w,b)$ is the cost for the given params.
	- And we want to minimize it such that we find parameters $\hat{w}, \hat{b} = \arg \min E(w, b)$:
$$
\frac{1}{n} \sum_{i=1}^n (y_i - (wx_i + b))^2, \quad \text{MSE}
$$

**Closed-Form Solution using MSE**
The simplest way is using vectors, which will give the length to the closest point along the line, it has to be orthogonal, and not just down the y axis like MSE:
$$
w=(X^TX)^{−1}X^Ty
$$
And using MSE:
1.  Partial derivative for b
$$
\frac{\partial E(w, b)}{\partial b} = -\frac{2}{n} \sum_{i=1}^n (y_i - w x_i - b)
$$
	- Set $\frac{\partial E}{\partial b} = 0$:
$$
b = \bar{y} - w \bar{x}
$$
2. Partial derivative for w
$$
\frac{\partial E(w, b)}{\partial w} = -\frac{2}{n} \sum_{i=1}^n x_i (y_i - w x_i - b)
$$
	- Substitute $b = \bar{y} - w \bar{x}$ and solve:
$$
w = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2}
$$
3. Final
$$
w = \frac{\text{Cov}(x, y)}{\text{Var}(x)}, \quad b = \bar{y} - w \bar{x}
$$

---
## Linear Models for Classification
*Using standard linear regression will not work for classes, so we use logistic regression*
+ We then use the likelihood of the y being class a given features x
+ The probability comes from applying the sigmoid function 

1. **Objective**:
    
    - Predict the probability $P(y=1∣x)P(y = 1 \mid x)$.
2. **Logistic Model**:
    - Logistic regression equation: $\log \frac{p}{1-p} = wx + b$
    - Equivalent probability representation: $P(y = 1 \mid x) = \sigma(wx + b)$
    - **Sigmoid Function**: $\sigma(z) = \frac{1}{1 + e^{-z}}$
        - Maps $z$ to probabilities in $[0,1]$.
3. **Classification Rule**:
    - Decision boundary: $wx + b = 0$.
    - Rules:
        - $wx + b > 0$: Predict $y = 1$.
        - $wx + b < 0$: Predict $y = 0$.



#### Conditional Likelihood:
*A little like Naive Bayes Classifier*

The likelihood of the observed data $(\mathbf{y} )$ given $( \mathbf{X})$ is:
$$
P(\mathbf{y} \mid \mathbf{X}) = \prod_{i=1}^n \sigma(wx_i + b)^{y_i} (1 - \sigma(wx_i + b))^{1-y_i}
$$
Where:
- $\sigma(wx_i + b)$: Predicted probability for $y_i = 1$.
- $1 - \sigma(wx_i + b)$: Predicted probability for $y_i = 0$.

#### Log-Likelihood:
Maximizing the log of the likelihood simplifies the computation:
$$
\ell(w, b) = \sum_{i=1}^n \left( y_i \log \sigma(wx_i + b) + (1 - y_i) \log (1 - \sigma(wx_i + b)) \right)
$$

#### Log-Loss:
Minimizing the **negative log-likelihood** gives the log-loss:
$$
L(w, b) = -\sum_{i=1}^n \left( y_i \log \sigma(wx_i + b) + (1 - y_i) \log (1 - \sigma(wx_i + b)) \right)
$$
- This will give values between 0 and $\infty$ something which is possible to minimize

#### Summary:
- Logistic regression learns parameters $w$ and $b$ by:
  - **Maximizing log-likelihood**, or
  - **Minimizing log-loss**.
- The log-loss penalizes incorrect predictions and is commonly used as the optimization objective in logistic regression.

---

# Gradient Decent
1. **Definition**:
    
    - An approximate method to find the local minimum of a multivariate function $f(x)$.
    - Decreases $f(x)$ fastest in the direction of the negative gradient $-\nabla f(x)$.
2. **Update Rule**:
    
    $a_{t+1} = a_t - \gamma_t \nabla f(a_t)$
    - $\gamma_t$: Learning rate (step size).
    - $\nabla f(a_t)$: Gradient of $f$ at $a_t$.

### Learning Rate
1. **Choosing $\gamma$**:
    
    - **Too small $\gamma$**: Slow convergence.
    - **Too large $\gamma$**: May "jump over" the local minimum.
2. **Illustration**:
    
    - Small learning rate: Gradual descent.
    - Large learning rate: Oscillation or divergence.

### Multivariate Linear Regression
1. **Goal**:
    
    - Learn a linear function $f: \mathbb{R}^d \to \mathbb{R}$.
    - Instead of a line, it learns a hyperplane.
2. **Representation**:
$$
\mathbf{w}^T \mathbf{x} = w^{(0)} + w^{(1)}x^{(1)} + \dots + w^{(d)}x^{(d)}
$$
    - $\mathbf{X} \in \mathbb{R}^{n \times d}$ is the design matrix (data matrix).
    - $\mathbf{y} \in \mathbb{R}^n$ contains corresponding labels.
    - Weights $\mathbf{w} \in \mathbb{R}^d$ are learned parameters.
3. **Trick**:
    - Add a constant term $x^{(0)} = 1$ to automatically include the intercept.

## Gradient Descent for Logistic Regression
1. **Objective**:
    - Minimize the loss function $L(w)$ for logistic regression.
2. **Derivative of Sigmoid**:
$$
\sigma'(z) = \sigma(z)(1 - \sigma(z))
$$
3. **Gradient of Log-Loss**:
$$
\nabla L(w) = \sum_{i=1}^n x_i (\sigma(\mathbf{w}^T x_i) - y_i)
$$
1. **Update Rule**:
$$    
\mathbf{w}_{t+1} = \mathbf{w}_t - \gamma_t \sum_{i=1}^n x_i (\sigma(\mathbf{w}_t^T x_i) - y_i)
$$

