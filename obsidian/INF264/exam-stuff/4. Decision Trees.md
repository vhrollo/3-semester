# Structure

**Inner nodes**
- Associated with a test
- Outgoing edges correspond to possible test results

**Childs**
- When generated, it is selected based on test results

**Leaf nodes**
- Classification
- Regression
- Probabilistic ig? 


### Expressiveness
*Can approximate any function of the input features*

Can be large, but wont probably generalize well
- if consistent, if no duplicates, it will be consistent every time

Prefer more compact decision trees ( Ockham's Razor )


### Learning
*Greedily split on a feature. We want to reduce the uncertainty of a split*

**Entropy** *Entropy measures average information content that is obtained from a stochastic information source* $$ H(x)=\sum_i P(x=x_i)\log_2P(x=x_i) $$ **Gini Index** - *alternative*
$$
H(x)=\sum_i P(x=x_i)(1-P(x=x_i))
$$

**Misclassifications** - alternative 
$$
3 = 1 - \max_iP(x = x_i)
$$

### **Conditional Entropy**
Conditional entropy quantifies the uncertainty in $y$ given knowledge of $x$:
$$
H(y \mid x) = -\sum_i P(x = x_i) \sum_j P(y = y_j \mid x = x_i) \log_2 P(y = y_j \mid x = x_i)
$$

1. **Outer Summation ( $\sum_i$):**
   - Loops over all possible values of $x$ (e.g., $x_1, x_2, \dots$).
   - $P(x = x_i)$: Probability of $x$ taking the value $x_i$ (acts as a weight for that value of $x$).

2. **Inner Summation ($\sum_j$):**
   - Calculates the entropy of $y$ for a specific $x = x_i$.
   - $P(y = y_j \mid x = x_i)$: Probability of $y$ being $y_j$ when $x = x_i$.

3. **Logarithmic Term:**
   - $\log_2 P(y = y_j \mid x = x_i)$: Measures the "unexpectedness" of $y_j$ given $x_i$. Smaller probabilities contribute more to the entropy.

4. **Negative Sign:**
   - Ensures that the entropy is a positive value, as probabilities $0 \leq P \leq 1$ lead to negative logarithms.



In decision trees, **information gain** measures how much $H(y)$ decreases after splitting on $x$. It's computed as:
$$
\text{info\_gain} = H(y) - H(y \mid x)
$$

---

### **Information Gain in Your Code**

1. **Split the Data**:
   - The feature $x$ is split at a `threshold` into `left_child` and `right_child` using a boolean mask. Where left and right are the two x that can happen

2. **Weighted Entropy**:
   - Calculate the size-based weights:
     $$ 
     \text{left\_weight} = \frac{\text{size of left\_child}}{\text{total size}}
     \quad \text{and} \quad 
     \text{right\_weight} = \frac{\text{size of right\_child}}{\text{total size}}
     $$
   - Compute the weighted entropy (criterion) for each split:
     $$ 
     H(y \mid x) = \text{left\_weight} \cdot H(\text{left\_child}) + \text{right\_weight} \cdot H(\text{right\_child})
     $$

3. **Compute Information Gain**:
   - Subtract the weighted entropy of the children from the total entropy:
     $$ 
     \text{info\_gain} = H(y) - H(y \mid x)
     $$

---


# ID3  - Iterative Dichotomizer 3

1. If all data points have the same label
	- return a leaf with that label
2. Else if all points have identical feature labels
	-  return leaf with the most common label
3. Else
	- Choose a feature that maximizes the information gain
	- Add a branch for each value of the feature
	- for each branch
		- call the algorithm recursively for all the data points with the particular feature value


### Overfitting
*Inductive bias: prefer smaller trees*
- Set a minimum size for a leaf or maximum depth for the tree
- Early stopping ( pre-pruning ):
	- Stop recursion once information gain is non-positive
- Pruning ( post-pruning ):
	- Build a full tree, remove parts of it later

### Pruning:
*Divide data into training and pruning (validation) data*
1. build a full tree
2. for each subtree T: if the subtree with maj label in T does not decrease accuracy then:
	- replace T with a leaf node that predicts the maj class

### Regression trees
- prediction values y using the points associated with the leave
	- typically by averaging
- learning
	- using MSE
	- information gain form a split is the MSE - MSE beforehand

### Properties of decision trees
- Pros
	- Fast
	- Easy to interpret
	- Invariant to scaling
	- both categorical and continuous
	- implicit features selection
- cons
	- unstable
		- due to greedy learning, small differences in training data can result totally different trees
	- usually competitive accuracy only in an esamble