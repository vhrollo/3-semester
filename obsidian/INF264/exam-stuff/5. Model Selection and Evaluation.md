# No free lunch theorem
*"When averaged over all possible datasets, all classifiers perform equally well" - Wolpert & Macready* 

- If an algorithm A performs better than B on a dataset D, then there must exist a dataset which B performs better than A
- We want to choose an algorithm which works well on datasets which are likeli to occur
	- Then the other algorithm is probably better at datasets we are not likeli to get

# Terminology
**Model selection**: Estimating performance of several models in order to choose the best one. This is a part of the training process
- We should generalize
- Data is regularity + noise
- We want to fit to the regularity, so we don't overfit
- Select by estimating prediction error on unseen data points
- Using statistics
    - AIC, BIC
    - heteroskedacicity
  - validation data

**Model evaluation**: Having chosen the final model, evaluating its prediction error on new data

**Training data**: Constructing models

**Validation data**: Selecting model and adjusting hyper parameters

**Test data**: Estimating performance on unseen data

**Cross-validation** - *Partition data into k equally-sized subsets*
- We then evaluate on each subset $D_i$, and then train on the rest
- We then combine from each val score
- We only need to split into train - test
- This will reduce variance

**Hyperparameters** - *Options that you set before running an algorithm*
- CV can be used on top of this, can get extensive
- Grid search
- Randomized Search

**Bootstrap** - *randomly sample n points with replacement*
- This way you get on average 63.2% of the train data points



# Validation error underestimates test error
- The model should only change when we are conducting training
- If one uses validation data for model selection then validation error is essentially training error
- Test data cannot be used multiple times


## Performance Measures
*Performance measure values are relative. What is good and what is not depends on the situation. Sanity checks*

- RMSE - Root Mean Squared Error
$$
\text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^n \left(y_i - \hat{f}(x_i)\right)^2}
$$
+ MSE  - Mean Absolute Error
$$
\text{MAE} = \frac{1}{n} \sum_{i=1}^n \left|y_i - \hat{f}(x_i)\right| 
$$
	- Less sensitive to outliers compared to RMSE because it does not square the errors.

- Confusion Matrix - Matrix which contains counts of true label-prediction class label pairs
	- Accuracy - points on the diagonal, correctly classified.
	- Precision - How many correct guesses of the positive guesses
	- Recall - How many correct guesses of the true guesses

- F1 score - *the best of both precision and recall*
$$
F_1 = 2 \cdot \frac{\text{precision * recall}}{\text{precision + recall}}
$$

- Weighted F1 score 
$$ 
\text{Weighted F1} = \sum_{i=1}^{C} w_i \cdot \text{F1}_i
$$
	- C is then the number of classes, so it will average it out proportionally with F1 score on each class
