# Multivariate linear regression
*Instead of making a line, we make a hyperplane*

**Data Matrix**:
*n instances $x_i^T$ as rows and $y \in \mathbb{R}^n$ has the corresponding label $y_i$*
- $X \in \mathbb{R}^{n x d}$ 
- $W \in \mathbb{R}^d$   
- Useful trick inn the features add $(1, x^{(1)}, x^{(2)})$ to add constant

**Error minimizing**
As we squared error, mean error squared could be used
$$
\sum_{i=1}^n \epsilon_i^2 = \sum_{i=1}^n (y_i - \mathbf{w}^T \mathbf{x}_i)^2 = \|\boldsymbol{\epsilon}\|_2^2
$$

**Using Linear alg.**
*this can be numerically stable, but there are other types of inversion*
$$
\hat{w} = (X^TX)^{-1}X^Ty
$$

---

## Loss functions
 **Mean Squared Loss**
 - sensitive to outliers

**Mean Absolute Loss**
- Less sensitive to outliers
$$\mid y - \hat{f}(x) \mid$$
---

# Non-Linearities
*Linear models are computationally convenient, but as simple models they are unable to capture complex non-linear relations*
- This problem can be solved using non-linear transformations

**Typically denoted as**:
$$
f(x) = W^T\phi(x)
$$
- Such transformation can be:
	- $\phi(x) = (1,x,x^2)$


## Basis functions

**Polynomial of degree k**
$$
\phi(x) = (1,x,x^2,\dots, x^k)
$$ 

**Radial basis functions**
$$
\phi(\mathbf{x}) = e^{-\epsilon \|\mathbf{x} - \mathbf{c}\|^2}
$$
- where c is the center point


Why do this
- Adds flexibility
- More places to slide in a hyperplane

Why not
- The dimensionality grows


---
###  Feature engineering
Use domain knowledge to construct transformations

### Overfitting
- Each polynomial with n-1 degree, will be able to fit perfectly to n data points

### Bias-Variance tradeoff
- A simple model will not be able to fit perfectly to the training data
- A complex model typically fits better
- **Bias**: Error due to modeling assumptions
- **Variance**: Error due to variations in the training set

- Simple models tend to have high bias, and low variance
- Complex models tend to have low bias, and high variance

**Context**
- A training set \( D \) contains \( n \) points sampled from \( P(x, y) \).
- Data is generated as:
  $$
  y_i = f(x_i) + \epsilon_i, \quad \epsilon_i \sim N(0, \sigma^2)
  $$
  - $f(x_i)$: True underlying function.
  - $\epsilon_i$: Noise (mean = 0, variance = $\sigma^2$).
- The model $\hat{f}_D(x)$ is learned from the training set D.

**Generalization Error**
The expectation of squared error over all possible training sets:
$$
E_D[(y - \hat{f}_D(x))^2]
$$
This measures how well the model generalizes to unseen data.

**Bias-Variance Decomposition**
The generalization error can be decomposed as:
$$
E_D[(y - \hat{f}_D(x))^2] = \text{Bias}^2 + \text{Variance} + \text{Irreducible Error}
$$

1. **Bias**:
   $$
   \text{Bias} = f(x) - E_D[\hat{f}_D(x)]
   $$
   - Error from **simplifying assumptions** (e.g., underfitting).

2. **Variance**:
   $$
   \text{Variance} = E_D[\hat{f}_D(x)^2] - (E_D[\hat{f}_D(x)])^2
   $$
   - Error from **model sensitivity to different training sets** (e.g., overfitting).
   - f.ex if decision tree changes drastically if given a somewhat diff training set

3. **Irreducible Error**:
   $$
   \sigma^2
   $$
   - Noise inherent in the data, which no model can eliminate.

**Key Insights**
- **Bias**: Low complexity models (e.g., underfitting) result in high bias.
- **Variance**: High complexity models (e.g., overfitting) result in high variance.
- **Irreducible Error**: Always present due to noise in the data.

**Diagnosis**
- High training error, high test error $\rightarrow$ underfitting
- Low training error, high test error $\rightarrow$ overfitting


---
# Regularization
*penalize complexity*

- Any objective which intends to reduce generalization error, but no the training error
- Using L2 reguralizer, where $\lambda$ is the parameter controlling the weight of the reguralizer
$$ \| (\mathbf{y} - \mathbf{w}^T \mathbf{X}^T) \|_2^2 + \lambda \| \mathbf{w} \|_2^2 $$
- more data points will give less of an effect
o