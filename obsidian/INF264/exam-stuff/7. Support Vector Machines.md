*classifier for linearly separable data*

**Institution**: Draw a decision boundary which maximizes the gap of points of either class (support vectors).

## Decision Boundary
- We use hyperplanes as a way to divide the space into two
- This is denoted as:
$$
w^Tx+b=0
$$
- where $w\in \mathbb{R}^d, b \in \mathbb{R}, y_i \in \{-1, 1\}$
- Rules:
	- $w^Tx + b > 0$: Predict $y = 1$.
	- $w^Tx + b < 0$: Predict $y = 0$.
	
Point on the boundary:
$\vec{w}^T \vec{x} - b = 0$ 

Hyperplane
$w_1x_1 + \ldots + w_p x_p - b = 0$

**The distance between the two hyperplanes**
- $w^Tx+b_1=0$
- $w^Tx+b_2=0$
- which can be written as $w^Tx+b= \pm 1$ 
- $D = \mid b_1 - b_2 \mid / \mid W \mid$ (length of W, which is the normal vector)

$\vec{w}^T (\vec{x}+k \frac{\vec{w}}{||w||}) - b = 1$

This is the same as:
$k \frac{\vec{w}\cdot \vec{w}}{||w||} = 1$

which means that k must be
$k = \frac{1}{||w||}$

then the margin must be:
$k = \frac{2}{||w||}$

Maximizing the margin
- We then need to minimize ||w||

This will lead to:
$y_i = 1 \Rightarrow X^T x + b \geq 1$ 
$y_i = -1 \Rightarrow X^T x + b \geq -1$ 

or shortened
$y_i (\vec{w}\vec{x_i}-b)\geq 1$

So we want to:
$$
\min_{w,b} \frac{1}{2}\mid\mid w\mid\mid ^2
$$


A loss function for this can be defined as a Hinge loss
$\mathcal{L} = max(0, 1=y_i ( w*x_i - b))$ 
which means if it goes the wrong way, it will be positive
and if it is under 1 it will b, and if it goes positive it will be negative, which means 0 is bigger

This basically means:
$$
l = 
\begin{cases} 
0 & \text{if } y \cdot f(x) \geq 1 \\
1 - y \cdot f(x) & \text{otherwise} 
\end{cases}
$$
which then can be derivated for b w or whatever function f(x) one is using partial derivation to find how it should move about:

And then we can add regularization on top of this: ( in this case Ridge regularization)
$$
J = \lambda ||w||^2 + \frac{1}{n} \sum_{i=1}^{n} \max(0, 1 - y_i (w \cdot x_i - b))
$$

if $( y_i \cdot f(x) \geq 1 )$:
$$
J_i = \lambda ||w||^2
$$

else:
$$
J_i = \lambda ||w||^2 + 1 - y_i (w \cdot x_i - b)
$$


# Kernel 

You don't have to to use the transformation to new higher

Dot product in some feature space to avoid projecting into higher dimensions