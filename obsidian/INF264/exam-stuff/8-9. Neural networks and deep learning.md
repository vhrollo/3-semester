
# From linear to non-linear

to extend linear models to represent non-linear functions, we can use a non-linear transform $\phi(x)$ 
1. Use a very generic high-dimensional $\phi$
	- polynomial features, radial bases function
1. Carefully choose $\phi$ (feature engineering)
	- image processing
	- requires more domain knowledge
2. Learn $\phi$. Now $y = f(x\mid \Theta, w) =\phi(x;\Theta)^Tw$. Transform $\phi$ is parameterized and we can find an optimization algorithm to find the parameter values
	- parameter $\Theta$ gets learned automatically through optimization
	- learning $\phi$ reduces  the need for manual feature engineering, as this is happening automatically
## Basic Idea
- Representation learning
- Traditional machine learning: feature engineering with basis functions
- Neural networks: learn features from data
	- Final classifier/regression model simple, e.g., logistic regression
		- simple patterns
	- Complex features by combining lots of simple (non-linear) transformations
		- f.ex faces in a picture

**The Perceptron / Artificial Neuron**
- Multiply