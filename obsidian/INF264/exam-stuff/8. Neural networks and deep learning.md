# From linear to non-linear

to extend linear models to represent non-linear functions, we can use a non-linear transform $\phi(x)$ 
1. Use a very generic high-dimensional $\phi$
	- polynomial features, radial bases function
1. Carefully choose $\phi$ (feature engineering)
	- image processing
	- requires more domain knowledge
2. Learn $\phi$. Now $y = f(x\mid \Theta, w) =\phi(x;\Theta)^Tw$. Transform $\phi$ is parameterized and we can find an optimization algorithm to find the parameter values
	- parameter $\Theta$ gets learned automatically through optimization
	- learning $\phi$ reduces  the need for manual feature engineering, as this is happening automatically
## Basic Idea
- Representation learning
- Traditional machine learning: feature engineering with basis functions
- Neural networks: learn features from data
	- Final classifier/regression model simple, e.g., logistic regression
		- simple patterns
	- Complex features by combining lots of simple (non-linear) transformations
		- f.ex faces in a picture

**The Perceptron / Artificial Neuron**
- Multiply each input with corresponding weight
- Take a sum of these products and feed it into a (non linear) activation function

*Outputs from one layer are inputs for the next layer*

**Notation**
- feedforward network specifies a mapping $y \approx f(x, \Theta)$, and learns parameters $\Theta$ that results in the best function approximation
- $x \in R^d$ input vector
- y can be one or multiple outputs
- $\Theta$ is weights
- feedforward:
	- $f(x) =f^{3}(f^{2}(f^{1}(x)))$

**Learning features**
*Let us consider a simple neural network with one hidden layer ( + one output layer)*
-  $f(x) =f^{2}(f^{1}(x))$
- $h=f^1(x)$
- output function can be a $y=v^Th+c$
- hidden uni $h=g(W^Tx+b)$
	- where W are weights and b intercepts (the constant added at each layer)
- W is used for a single layer, whilst $\Theta$ is used for *all* weights

---
# Output units

**Linear units for continuous output**:
- Given features $h, \hat{y} = W^Th + b$ 
**Sigmoid**
- for binary output
**Softmax units for multinominal outputs**
- $z = W^Th + b$
$$p(y =j|x) = \frac{exp(z_i)}{\sum_jexp_j(z_j)}$$
- if one want to find the most likeli class, you do an arg max:
$$\hat{y} = \arg\max_j p(y = j \mid x)$$

---
### Activation functions

**Traditional activation**
- Sigmoid
- Tanh

**Modern**
- Relu
- Leaky relu
- Expo LU

### Architecture
- Already a neural network with one hidden layer is a universal approximator
	- can approximate any arbitrary function with ish precision given it has enough neurons
- several hidden layers allows for composing complex features using simpler features from earlier layers


### Cost functions

- Let $D$ be our training data.
- For regression, we can minimize squared loss:
  $$
  L(\theta) = \sum_{(x, y) \in D} \|y - f(x \mid \theta)\|^2
  $$

**Log-loss (for classification):**
  $$L(\theta) = -\sum_{(x, y) \in D} \left( y_i \log f(x \mid \theta) + (1 - y_i) \log(1 - f(x \mid \theta)) \right)$$
- if average loss $\frac{1}{N}$

- **How it works:**
  - Penalizes incorrect predictions:
    - If $y_i = 1$, uses $\log f(x \mid \theta)$ (penalty increases as $f(x \mid \theta) \to 0$).
    - If $y_i = 0$, uses $\log(1 - f(x \mid \theta))$ (penalty increases as $f(x \mid \theta) \to 1$).
  - Encourages probabilities $f(x \mid \theta)$ to align with true labels.

- **Key points:**
  - Lower loss = better predictions.
  - Differentiable, ideal for gradient-based optimization.
  - Outputs well-calibrated probabilities for decision-making.


---
### Learning
- forward: performance inference
- backward: performance learning
- back-propagation
- Stochastic Gradient Decent (SGD)

**Gradient-based learning**
- Goal is to minimize training loss by selecting appropriate values for the weights
- Update weights using gradient decent: stochastic is doing it with an subset
$$\min_{\theta} L(\theta)$$ 
- Update weights using gradient decent:
$$\theta_{t+1} = \theta_t - \gamma_t \nabla L(\theta_t)$$


**Back-propagation**
- backward propagation of errors
- An efficient algorithm for computing the gradient
- Apply the chain rule repeatedly

- adversarial attack
	- one changes the label and then it back-propagates to see how it changes the input data



---
### **Stochastic Gradient Descent (SGD)**

#### **Gradient of the Loss Function:**
$$
\nabla L(\theta) = \sum_{i=1}^n \nabla L_i(\theta)
$$
- $L_i(\theta)$: Loss for the $i$-th data point.
- Computing the gradient for large $n$ (number of data points) can be slow.

#### **Key Idea:**
- Instead of computing the exact gradient over all data points, SGD computes an **approximate (noisy) gradient** over a **subset (minibatch)** of data.
- This speeds up training by taking **smaller, noisier steps** in the right direction.

---

### **SGD Steps:**
1. **Sample a minibatch** of $m$ training examples.
2. **Compute gradient** on the minibatch:
   - **Forward pass:** Compute the loss.
   - **Backward pass:** Compute the gradient of the loss.
3. **Update weights** using the gradient.

---

### **Minibatches:**
- **Size considerations:**
  - Often a power of 2 (e.g., 32, 64) for GPU efficiency.
  - Small minibatch sizes add regularization (e.g., 1 is too noisy).
- **Epoch:**
  - One complete pass through the training data.
  - $n/m$ SGD steps are required per epoch, where $n$ is the dataset size and $m$ is the minibatch size.

---

### **Summary:**
- SGD approximates gradients for faster, scalable optimization.
- Minibatch size impacts performance:
  - Larger: Smoother updates.
  - Smaller: Faster but noisier updates.

