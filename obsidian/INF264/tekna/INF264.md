What is a model
- something guessing y form x
Arcem razer theorem
- simpler is better - based fr


## Performance measure
- Performance measure is problem-specific
- loss vs performance
- could be misleading due to balanced vs normal

**Types of performance measures**
- Accuracy
- Precision/Recall/F1score
- Confusion matrix

Regression
- worse redictions have a linear 

**more**
- Recall
	- punishes false positive
	- criminal conviction 
- Recall
	- punishes false negative
	- cancer detection
- f1 score
	- combines both
	- good alternative to accuracy when labels are unevely distributed


# Bias

**Prejudice dependent bias**
- inductive bias
	- false assumption
- bias to seen data
	- making choice based on data
- sampling bias
	- how the data is collected

**Learned bias**
- Bias term in linear regression

**Variance**
- Model performance
	- how much does model performance change with repeated trials
		- randomness in learning algorithm
		- small fluctuations in training data


**Bias-variance tradeoff**
- something


# Model selection and something


**Training data**
- Should be representative sample of the real world
	- If its ont representative, we have sampling data bias

Validation data
- Used to compare several models
	- Biases our model towards validation data
- Used to tune and find the best hyperparameters

- Hold out validation
	- use a single, already chosen validation dataset
	- often overestimates

Test data
- Represent actual "unseen" data

Testing on test data should only be done once
- this minimizes the bias introduced when selecitng for performance

# Model Selection Pipeline
- First split the dta
- Only use val for eval
- only use test for final eval

- Decisions should only be made from training data


**Cross-Validation**
- An alternative to hold-out validation
- Skips explicit split to validation dataset
- instead splits into many different training/validation sets
- Lowers the variance in esitimated preformance


Data leakage
- Training data into test data
	- Oversampling before splitting
- Test data into training data
	- normalizing on the wole dataset


# Important concepts

**Dimensionality**
- each feature
- curse of dimensionality
	- high dimensional data can be problematic
	- similarity - less variance in distance between datapoints
	- sparsity - we need more datapoints to get good coverage
- dimensionality reduction
	- feature selection
		- finds features to ignore
	- feature engierring
		- compute new features
	- PCA, Autoencoder, clustering

**No free luch theorem** has been on every exam
- The theore asserte that when the perforamnce of all optimization methods is avaraged across all concevable problems, they all perform equally

**Data preprocessing**
- alter dta to make learning easier
- scaling data
- feature egineering
- featue selection
- balancing data
	- over-/undersampling
	- generating samples?

**Overfitting**
- Our models ar fit to the noise, from the training data

Underfitting
- Our models do not capture the relationship between input and output
-  transform through kernels


# Models and Learning algorithms

### K-Nearest Neighbors
- Stores training data in a huge dataset
- finds the k-nearest neighbours

- Hyperparameters
	- k - how many neighbours is democratically is voting

- Assumptions
	- WE can predict label only given closest neighbours

- Strength
	- no training time
	- roubust to outliers
- weakness
	- higher inference time
	- memory intensive
	- bad extrapolation data it has not seen



Scaling vs Normalization
- normalization reducing the mean to 0 and reducing the variance around
- scaling just devise so deviding/multiplying



# Linear Regression

- Given training data, find a linear function which minimizes error

- assumtionns
	- there is a relationship between input and output
-
- Strength
	- low complexity
	- fast training and inference
	-
- Can come with high bias

**Basis function**
- hyperparameters could also be thought of as preprocessing
- Non-linear transformation of data
- kind of an feature engineering step
- can be everything

**Logistic Regression**
- Algorithm
	- Binary classification
	- Gives us a probabilty
		- If over 0.5 it belongs to positie class,
		- iv under it belongs to negative class
	- One vs rest, or one vs all
	- No closed form solution, uses gradient descent
- hyperparms
	- basis function
	- reguralizaiton

- exam
	- You need to be able to solve a regression based on visual aid

# SVM
Algorithm
- finds hyperplane that splits teh space in two, and maximizes the margin between two classes
- uses kernels (basis functions) to transofrm data
- C - soft margin
	- how soft should the bargin be

Assumptions
- Linear separability
- inofrmation lies between the support vectors

Strengths
- high complexity

weaknesses
- strong

**SVM prediction**
- basically check if value is over or under 0
- if 0 it is 50/50


# Naive Bayes
Algorithm:
- Works best with categorical features

Hyperparams
- different types of distribution for events

Assumptions
- All features are independent
- Feature independence usually doesnÃ¦t hold

Strength
- Very simple and fast
- Can handle missing data
- small datasets

weaknes
- assumptions rarely holds
- encountering a feature outside the training data given label 0 probability

**Prediction**
$P(banana) \cdot P(Long\mid banana) \cdot P(yellow\mid banana)$


# Neural Networks

Algorithm
- back-propagation and gradient descent
- activation function, architecture, learning rate, reguralization

Assumptions:
- stability over the space of parameters, loss function is smooth

Strength
- Universal approximators
- infinitevly complex
- veratile

Weakness
- requires alot of data
- not very explainable -> black box

18
12

9
-12

Back-propegation


# Gradient Descent
- A numerical method used for finding a local minima
- Given a weight $w_0 = 3$, we have found that $\frac{dL}{dw}=2$, learning rate 0.1
- 3 - 0.1 * 2 



# Decision Tree
Algorithm
- splittihg data into subsets
- classification, but can also be used for regression
- ID3

ASsumption
- the labels can be done by splitting the feature value

Strength
- fast
- explanibility is inherent
	- a given prediction is just a set of yes/no questions

Weakness
- High variance
- Computationally expensive

# Esamble Methods
Algorithm
- reduces varaince
- prodces better results
 - the weak models need to be suffiently different
 - can be done with any learning algorithm


- If each model is nidependent and better than random, we get the graph on the right
- However this is hard

- Two main esamble method
	- Bagging - Sampling from the entire dataest with replacement
		- some datapoints will be sampled multiple times
		- 63% will be included
		- train the model on this trainin subset
		- add it to the ensemble
		- Random forests
	- Boosting
		- Weight those datapoints more in the next iteration
		- When predicting, we weight each model according to the weight calculated
		- sequential
		- reduces bias, unlike bagging
			- can also reduce variance





# Random Forests
- Several weak decision trees
- ensamble
- limit each tree to increase diversity
	- only let it see a subset of features


Assumptions:
- Scale is not important
- Each decision tree is independent

Strength:
- Very robust against overfitting
- still retains low veriance

Weakness
- We loose the explainibility 


# PCA
*Dimensionality reduction methods*

- Finds the most interesting featues 
- Computed by finding the eigenvectors of the covariance matrix



# AutoEncoder
- Scale data down to bottleneck
- Reconstruct from this encoder
- After training only keep encoder part
- KETIL SEEMS TO LIKE THEM

# Clustering
*Unsupervised classification*

**Four main categories**
- Centroid
- Density
- Connection



**K-means**
- it assumes that the clusters are spherical

**Loyds algorithm**
- You choose k different possitions
- Assign every point to its closest cluster
- recompute cluster centroids based on the average position
- reapeat 2-3 times

**Hierarchical Clustering**
- Agglomerative
	- Bottom-up
- Divising
	- Top down

- No need to know how many clusters before hand


**Linkage functions**
- Single linkage
	- compare two closest neighbours in separate clusters
- complete linkage
	- compare two furthest neighbours in separate clusters
- average linkage
	- avarerage between every point in the different clusters

**Loyds algorithm**:
- do the algorithm


**hierarchical Clustering**
- Compute the distance matrix



# Missing data

**Missing completely at random (mcar)**
- doesn't depend on anything

Missing at random (MAR)
- depends on the value of a different variable

Missing not at random (MNAR)
- depends on the value of itself

Possible solution
- Impute values
	- avaarage from the training set
	- model it from relationship with other features
		- regression
	- however can bias
- use models which

# No free lunch theorem
*There exist no algorithm better than any other*



# Possible exam topics
- overfitting underfitting evaluate performance
- no lunch theorem
- some dimensionality reduction
- clustering
- gradient decent
- 

# Exams H20

**F1**:
Machine learning cannot be done without assumptions, maybe try another model than linear.

**F2**:
Alice is way to simple for the task. Alice is underfitting to the data. You have a very low training and validatoin accuracy

Bob is overfitting to the data. Training accuracy is high, while validation accuracy is low

**F3**:
No free lunch theorem. 

**F4**:
Bob has right, but K-means is spherical algorithm. Hiercical clustering, density clustering, or GMM

**F5**:
Bob just guesses everything as working, which is not balanced.

**F6**:
The red should be the first principal component. They need to be orthogonal as well. The magnitude as well